{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09. Enhanced Prophet v2: 2023ë…„ ì‹¤íŒ¨ ë³´ì™„\n",
    "\n",
    "---\n",
    "\n",
    "## ê°œì„ ì‚¬í•­ (v1 ëŒ€ë¹„)\n",
    "\n",
    "### 1. ê±°ì‹œê²½ì œ í”¼ì²˜ ì¶”ê°€\n",
    "- **DGS10** (10ë…„ ê¸ˆë¦¬): level, change_20d, zscore_252\n",
    "- **WTI** (ì›ìœ  ê°€ê²©): log, mom_3M, change_20d\n",
    "- **íš¨ê³¼**: ê¸ˆë¦¬ ê¸‰ë“±/ìœ ê°€ ë³€ë™ì— ë”°ë¥¸ ì„¹í„° ì˜í–¥ ë°˜ì˜\n",
    "\n",
    "### 2. ë ˆì§ í”¼ì²˜ ì¶”ê°€\n",
    "- **Market_Vol**: ì‹œì¥ ì „ì²´ ë³€ë™ì„±\n",
    "- **CrossSection_Dispersion**: ì„¹í„° ê°„ ìˆ˜ìµë¥  ë¶„ì‚°\n",
    "- **íš¨ê³¼**: 2023ë…„ ê°™ì€ ë ˆì§ ì „í™˜ ì‹œê¸° íƒì§€\n",
    "\n",
    "### 3. ë‹¨ê¸° ëª¨ë©˜í…€ ê°•í™”\n",
    "- **Mom_1M**: 1ê°œì›” ìˆ˜ìµë¥ \n",
    "- **Mom_6M**: 6ê°œì›” ìˆ˜ìµë¥ \n",
    "- **Mom_Accel**: ê°€ì†ë„ (1M - 6M)\n",
    "- **íš¨ê³¼**: 2023 Tech ê¸‰ë°˜ë“± í¬ì°©\n",
    "\n",
    "### 4. Prophet cps ë™ì  ì¡°ì •\n",
    "- ì„¹í„° ë³€ë™ì„± ê¸°ë°˜ ìë™ ì¡°ì •\n",
    "- high vol (>0.40): cps=0.10\n",
    "- mid vol (0.30~0.40): cps=0.05\n",
    "- low vol (<0.30): cps=0.03\n",
    "\n",
    "### 5. XGBoost Early Stopping\n",
    "- train/val split (80/20)\n",
    "- early_stopping_rounds=20\n",
    "- **íš¨ê³¼**: Energy ê³¼ì í•© ë°©ì§€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "from prophet import Prophet\n",
    "import xgboost as xgb\n",
    "import holidays\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ê°œì„ ëœ í”¼ì²˜ ëª¨ë“ˆ import\n",
    "from sector_rotation_features import (\n",
    "    load_and_prepare_macro,\n",
    "    merge_macro_to_sector_panel,\n",
    "    add_regime_features,\n",
    "    add_momentum_features,\n",
    "    calculate_dynamic_cps\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸\n",
    "import koreanize_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path('.').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / 'Data_set'\n",
    "OUTPUT_DIR = DATA_DIR / 'Enhanced_Prophet_v2_Results'\n",
    "TABLEAU_DIR = DATA_DIR / 'Tableau_Csv'\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"í”„ë¡œì íŠ¸ ê²½ë¡œ: {PROJECT_ROOT}\")\n",
    "print(f\"ê²°ê³¼ ì €ì¥: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2. ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ì£¼ì‹ ë°ì´í„° ë¡œë“œ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "df = pd.read_csv(DATA_DIR / 'stock_features_clean.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "df = df.dropna(subset=['Date', 'Close', 'Sector', 'Daily_Return'])\n",
    "df = df[df['Sector'] != 'Unknown']\n",
    "\n",
    "print(f\"\\n ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  ê¸°ê°„: {df['Date'].min().date()} ~ {df['Date'].max().date()}\")\n",
    "print(f\"  ê¸°ì—… ìˆ˜: {df['Company'].nunique()}\")\n",
    "print(f\"  ì„¹í„° ìˆ˜: {df['Sector'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê±°ì‹œê²½ì œ ë°ì´í„° ë¡œë“œ (ê°œì„ ì‚¬í•­ #1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 1ì˜ ì²« ë²ˆì§¸ ë‹¨ê³„: ê±°ì‹œê²½ì œ ë°ì´í„° ë¡œë“œ ë° íŒŒìƒ í”¼ì²˜ ìƒì„±\n",
    "macro_df = load_and_prepare_macro(DATA_DIR, verbose=True)\n",
    "\n",
    "print(\"\\nìƒì„±ëœ ê±°ì‹œê²½ì œ í”¼ì²˜:\")\n",
    "print(macro_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3. ì„¹í„° ì¸ë±ìŠ¤ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sector_index_log(df):\n",
    "    \"\"\"ìˆ˜ìµë¥  ê¸°ë°˜ ì„¹í„° ì¸ë±ìŠ¤ ê³„ì‚° (log ë³€í™˜)\"\"\"\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # ì´ìƒì¹˜ í´ë¦¬í•‘\n",
    "    upper_limit = df_clean['Daily_Return'].quantile(0.99)\n",
    "    lower_limit = df_clean['Daily_Return'].quantile(0.01)\n",
    "    df_clean['Daily_Return'] = df_clean['Daily_Return'].clip(lower=lower_limit, upper=upper_limit)\n",
    "\n",
    "    # ì„¹í„°/ì¼ìë³„ ì§‘ê³„\n",
    "    sector_daily = df_clean.groupby(['Date', 'Sector'], as_index=False).agg({\n",
    "        'Daily_Return': 'mean',\n",
    "        'Volume': 'sum',\n",
    "        'Company': 'count'\n",
    "    })\n",
    "    \n",
    "    sector_daily = sector_daily.sort_values(['Sector', 'Date'])\n",
    "    sector_daily['Daily_Return'] = sector_daily['Daily_Return'].fillna(0)\n",
    "    \n",
    "    # ì§€ìˆ˜ ê³„ì‚°\n",
    "    sector_daily['Index'] = sector_daily.groupby('Sector')['Daily_Return'].transform(\n",
    "        lambda x: 100 * (1 + x).cumprod()\n",
    "    )\n",
    "    \n",
    "    # log ë³€í™˜\n",
    "    sector_daily['y'] = np.log(sector_daily['Index'])\n",
    "    sector_daily['ds'] = sector_daily['Date']\n",
    "    \n",
    "    return sector_daily\n",
    "\n",
    "sector_df = calculate_sector_index_log(df)\n",
    "\n",
    "print(f\"\\n ì„¹í„° ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {sector_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4. Feature Engineering (ê°œì„ ëœ ë²„ì „)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ì¡´ í”¼ì²˜ í•¨ìˆ˜ (v1ê³¼ ë™ì¼)\n",
    "def build_sector_panel_from_stock_df(df, sector_index_df, key_columns, prefix=\"Avg_\", fill_method=\"ffill_bfill\"):\n",
    "    \"\"\"ì„¹í„° íŒ¨ë„ ìƒì„± (ê¸°ì¡´ ë¡œì§)\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "    sector_index_df = sector_index_df.copy()\n",
    "    if \"Date\" not in sector_index_df.columns and \"ds\" in sector_index_df.columns:\n",
    "        sector_index_df[\"Date\"] = pd.to_datetime(sector_index_df[\"ds\"])\n",
    "    else:\n",
    "        sector_index_df[\"Date\"] = pd.to_datetime(sector_index_df[\"Date\"])\n",
    "\n",
    "    # ê¸°ë³¸ ì§‘ê³„\n",
    "    mean_cols = [c for c in key_columns if c in df.columns]\n",
    "    \n",
    "    if mean_cols:\n",
    "        sector_feats = df.groupby([\"Date\", \"Sector\"], as_index=False)[mean_cols].mean()\n",
    "        \n",
    "        rename_map = {c: f\"{prefix}{c}\" for c in mean_cols}\n",
    "        sector_feats = sector_feats.rename(columns=rename_map)\n",
    "        \n",
    "        sector_panel = sector_index_df.merge(sector_feats, on=[\"Date\", \"Sector\"], how=\"left\")\n",
    "    else:\n",
    "        sector_panel = sector_index_df.copy()\n",
    "\n",
    "    # ì •ë ¬ + ê²°ì¸¡ ì²˜ë¦¬\n",
    "    sector_panel = sector_panel.sort_values([\"Sector\", \"ds\"])\n",
    "\n",
    "    if fill_method == \"ffill_bfill\":\n",
    "        sector_panel = sector_panel.groupby(\"Sector\", group_keys=False).apply(\n",
    "            lambda x: x.ffill().bfill(),\n",
    "            include_groups=False\n",
    "        )\n",
    "\n",
    "    return sector_panel\n",
    "\n",
    "# í‚¤ ì»¬ëŸ¼ ì •ì˜\n",
    "key_columns = [\n",
    "    'Daily_Return', 'Return_1M', 'Return_3M', 'Return_6M',\n",
    "    'Volatility_20d', 'RSI_14', 'BB_Width',\n",
    "    'Vol_MA_20', 'Vol_Ratio', 'Vol_Z_Score',\n",
    "    'Drawdown', 'MDD'\n",
    "]\n",
    "\n",
    "sector_panel = build_sector_panel_from_stock_df(\n",
    "    df=df,\n",
    "    sector_index_df=sector_df,\n",
    "    key_columns=key_columns,\n",
    "    prefix=\"Avg_\"\n",
    ")\n",
    "\n",
    "print(f\" ê¸°ë³¸ ì„¹í„° íŒ¨ë„ ìƒì„±: {sector_panel.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ê°œì„ ëœ í”¼ì²˜ ì ìš© (ë°©ë²• 1ì˜ í•µì‹¬)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ê°œì„ ëœ í”¼ì²˜ ì ìš©\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. ê±°ì‹œê²½ì œ ë°ì´í„° ë³‘í•©\n",
    "print(\"\\n[1/3] ê±°ì‹œê²½ì œ ë°ì´í„° ë³‘í•©\")\n",
    "sector_panel = merge_macro_to_sector_panel(sector_panel, macro_df, verbose=True)\n",
    "\n",
    "# 2. ë ˆì§ í”¼ì²˜ ì¶”ê°€\n",
    "print(\"\\n[2/3] ë ˆì§ í”¼ì²˜ ì¶”ê°€\")\n",
    "sector_panel = add_regime_features(sector_panel, verbose=True)\n",
    "\n",
    "# 3. ëª¨ë©˜í…€ í”¼ì²˜ ì¶”ê°€\n",
    "print(\"\\n[3/3] ëª¨ë©˜í…€ í”¼ì²˜ ì¶”ê°€\")\n",
    "sector_panel = add_momentum_features(sector_panel, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" ëª¨ë“  ê°œì„  í”¼ì²˜ ì ìš© ì™„ë£Œ\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"ìµœì¢… Shape: {sector_panel.shape}\")\n",
    "print(f\"\\nìƒˆë¡œ ì¶”ê°€ëœ ì»¬ëŸ¼:\")\n",
    "new_cols = [c for c in sector_panel.columns if any(x in c for x in ['DGS10', 'WTI', 'Market_Vol', 'CrossSection', 'Mom_'])]\n",
    "for col in new_cols:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5. íœ´ì¥ì¼ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_market_holidays_simple(start_year, end_year):\n",
    "    nyse_holidays = holidays.NYSE(years=range(start_year, end_year + 1))\n",
    "    \n",
    "    holiday_data = []\n",
    "    for date, name in nyse_holidays.items():\n",
    "        holiday_data.append({\n",
    "            'holiday': 'market_closed',\n",
    "            'ds': pd.to_datetime(date),\n",
    "            'lower_window': 0,\n",
    "            'upper_window': 0\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(holiday_data)\n",
    "\n",
    "US_HOLIDAYS = get_market_holidays_simple(2018, 2026)\n",
    "print(f\"ë¯¸êµ­ ì‹œì¥ íœ´ì¥ì¼: {len(US_HOLIDAYS)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6.  ê°œì„ ëœ ëª¨ë¸ í•¨ìˆ˜ (ë°©ë²• 2 + 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 ê°œì„ ëœ Prophet ëª¨ë¸ (ë°©ë²• 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_prophet_model(holidays_df, sector_data=None, base_cps=0.05):\n",
    "    \"\"\"\n",
    "    ê°œì„ ëœ Prophet ëª¨ë¸ (v2)\n",
    "    \n",
    "    ê°œì„ ì‚¬í•­:\n",
    "    - ë™ì  cps ì¡°ì • (ì„¹í„° ë³€ë™ì„± ê¸°ë°˜)\n",
    "    - ê±°ì‹œê²½ì œ regressor ì¶”ê°€\n",
    "    - ë ˆì§ regressor ì¶”ê°€\n",
    "    - ëª¨ë©˜í…€ regressor ì¶”ê°€\n",
    "    \"\"\"\n",
    "    \n",
    "    # ë™ì  cps ê³„ì‚°\n",
    "    if sector_data is not None:\n",
    "        cps = calculate_dynamic_cps(sector_data, base_cps=base_cps)\n",
    "    else:\n",
    "        cps = base_cps\n",
    "    \n",
    "    model = Prophet(\n",
    "        growth='linear',\n",
    "        changepoint_prior_scale=cps,  #  ë™ì  ì¡°ì •\n",
    "        seasonality_mode='additive',\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False,\n",
    "        holidays=holidays_df,\n",
    "        interval_width=0.95\n",
    "    )\n",
    "    \n",
    "    # ê¸°ì¡´ ê¸°ìˆ ì  ì§€í‘œ\n",
    "    tech_regressors = [\n",
    "        'Avg_Volatility_20d',\n",
    "        'Avg_RSI_14',\n",
    "        'Avg_Vol_Z_Score',\n",
    "        'Avg_BB_Width',\n",
    "        'Avg_Return_3M'\n",
    "    ]\n",
    "    \n",
    "    #  ê±°ì‹œê²½ì œ regressor\n",
    "    macro_regressors = [\n",
    "        'DGS10_level',\n",
    "        'DGS10_change_20d',\n",
    "        'DGS10_zscore_252',\n",
    "        'WTI_log',\n",
    "        'WTI_mom_3M',\n",
    "        'WTI_change_20d'\n",
    "    ]\n",
    "    \n",
    "    #  ë ˆì§ regressor\n",
    "    regime_regressors = [\n",
    "        'Market_Vol',\n",
    "        'CrossSection_Dispersion'\n",
    "    ]\n",
    "    \n",
    "    #  ëª¨ë©˜í…€ regressor\n",
    "    momentum_regressors = [\n",
    "        'Mom_1M',\n",
    "        'Mom_6M',\n",
    "        'Mom_Accel'\n",
    "    ]\n",
    "    \n",
    "    all_regressors = tech_regressors + macro_regressors + regime_regressors + momentum_regressors\n",
    "    \n",
    "    for r in all_regressors:\n",
    "        model.add_regressor(r)\n",
    "    \n",
    "    return model, cps\n",
    "\n",
    "print(\" Enhanced Prophet ëª¨ë¸ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 ê°œì„ ëœ XGBoost ëª¨ë¸ (ë°©ë²• 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_enhanced_xgb_model(train_data, prophet_forecast, feature_cols=None, use_early_stopping=True):\n",
    "    \"\"\"\n",
    "    ê°œì„ ëœ XGBoost ì”ì°¨ ëª¨ë¸ (v2)\n",
    "    \n",
    "    ê°œì„ ì‚¬í•­:\n",
    "    - ê±°ì‹œê²½ì œ í”¼ì²˜ ì¶”ê°€\n",
    "    - ë ˆì§ í”¼ì²˜ ì¶”ê°€\n",
    "    - ëª¨ë©˜í…€ í”¼ì²˜ ì¶”ê°€\n",
    "    -  Early Stopping ì¶”ê°€\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prophet ì˜ˆì¸¡ê°’ ë³‘í•©\n",
    "    merged = train_data.merge(\n",
    "        prophet_forecast[['ds', 'Sector', 'yhat']],\n",
    "        on=['ds', 'Sector'],\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # ì”ì°¨ ê³„ì‚°\n",
    "    merged['residual'] = merged['y'] - merged['yhat']\n",
    "    \n",
    "    #  ê°œì„ ëœ í”¼ì²˜ ë¦¬ìŠ¤íŠ¸ (ë°©ë²• 3)\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [\n",
    "            'yhat',\n",
    "            \n",
    "            # ê¸°ì¡´ ê¸°ìˆ ì  ì§€í‘œ\n",
    "            'Avg_Volatility_20d',\n",
    "            'Avg_RSI_14',\n",
    "            'Avg_Vol_Z_Score',\n",
    "            'Avg_BB_Width',\n",
    "            'Avg_Return_3M',\n",
    "            \n",
    "            #  ê±°ì‹œê²½ì œ í”¼ì²˜\n",
    "            'DGS10_level',\n",
    "            'DGS10_change_20d',\n",
    "            'DGS10_zscore_252',\n",
    "            'WTI_log',\n",
    "            'WTI_mom_3M',\n",
    "            'WTI_change_20d',\n",
    "            \n",
    "            #  ë ˆì§ í”¼ì²˜\n",
    "            'Market_Vol',\n",
    "            'CrossSection_Dispersion',\n",
    "            \n",
    "            #  ëª¨ë©˜í…€ í”¼ì²˜\n",
    "            'Mom_1M',\n",
    "            'Mom_6M',\n",
    "            'Mom_Accel'\n",
    "        ]\n",
    "    \n",
    "    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì‚¬ìš©\n",
    "    feature_cols = [c for c in feature_cols if c in merged.columns]\n",
    "    \n",
    "    # ê²°ì¸¡ì¹˜ ì œê±°\n",
    "    merged = merged.dropna(subset=feature_cols + ['residual'])\n",
    "    \n",
    "    X = merged[feature_cols].values\n",
    "    y = merged['residual'].values\n",
    "    \n",
    "    # ìŠ¤ì¼€ì¼ë§\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    #  Early Stopping ì ìš©\n",
    "    if use_early_stopping and len(X_scaled) > 100:\n",
    "        # Train/Val Split (80/20)\n",
    "        val_size = int(len(X_scaled) * 0.2)\n",
    "        X_train, X_val = X_scaled[:-val_size], X_scaled[-val_size:]\n",
    "        y_train, y_val = y[:-val_size], y[-val_size:]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(\n",
    "            n_estimators=600,\n",
    "            learning_rate=0.03,\n",
    "            max_depth=3,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=2.0,\n",
    "            min_child_weight=5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        #  Early Stopping\n",
    "        xgb_model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            early_stopping_rounds=20,\n",
    "            verbose=False\n",
    "        )\n",
    "    else:\n",
    "        # Early Stopping ì—†ì´\n",
    "        xgb_model = xgb.XGBRegressor(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=3,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        xgb_model.fit(X_scaled, y)\n",
    "    \n",
    "    return xgb_model, scaler, feature_cols\n",
    "\n",
    "print(\" Enhanced XGBoost ëª¨ë¸ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 í†µí•© ì˜ˆì¸¡ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sector_year_enhanced(sector_data, year, holidays_df):\n",
    "    \"\"\"\n",
    "    ê°œì„ ëœ ë…„ë„ë³„ ì˜ˆì¸¡ (Prophet v2 + XGBoost v2)\n",
    "    \"\"\"\n",
    "    \n",
    "    sectors = sector_data['Sector'].dropna().unique()\n",
    "    if len(sectors) != 1:\n",
    "        raise ValueError(f\"sector_data must contain exactly 1 sector\")\n",
    "    sector_name = sectors[0]\n",
    "\n",
    "    df = sector_data.copy()\n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "    df = df.sort_values('ds').reset_index(drop=True)\n",
    "\n",
    "    split_date = pd.Timestamp(f\"{year}-01-01\")\n",
    "    next_year_date = pd.Timestamp(f\"{year+1}-01-01\")\n",
    "\n",
    "    train = df[df['ds'] < split_date].copy()\n",
    "    test = df[(df['ds'] >= split_date) & (df['ds'] < next_year_date)].copy()\n",
    "\n",
    "    if len(train) == 0 or len(test) == 0:\n",
    "        return None\n",
    "\n",
    "    # 1.  Enhanced Prophet (ë™ì  cps, ê±°ì‹œê²½ì œ regressor)\n",
    "    prophet_model, used_cps = create_enhanced_prophet_model(holidays_df, train)\n",
    "    \n",
    "    # Prophet ì…ë ¥ ì»¬ëŸ¼\n",
    "    prophet_cols = ['ds', 'y'] + [\n",
    "        'Avg_Volatility_20d', 'Avg_RSI_14', 'Avg_Vol_Z_Score', 'Avg_BB_Width', 'Avg_Return_3M',\n",
    "        'DGS10_level', 'DGS10_change_20d', 'DGS10_zscore_252',\n",
    "        'WTI_log', 'WTI_mom_3M', 'WTI_change_20d',\n",
    "        'Market_Vol', 'CrossSection_Dispersion',\n",
    "        'Mom_1M', 'Mom_6M', 'Mom_Accel'\n",
    "    ]\n",
    "    prophet_cols = [c for c in prophet_cols if c in train.columns]\n",
    "    \n",
    "    prophet_model.fit(train[prophet_cols])\n",
    "    \n",
    "    # Test ì˜ˆì¸¡\n",
    "    test_forecast_cols = [c for c in prophet_cols if c in test.columns]\n",
    "    prophet_forecast = prophet_model.predict(test[test_forecast_cols])\n",
    "    \n",
    "    # 2.  Enhanced XGBoost (Early Stopping, ê±°ì‹œê²½ì œ í”¼ì²˜)\n",
    "    # Train ì „ì²´ì— ëŒ€í•´ Prophet ì˜ˆì¸¡ í•„ìš”\n",
    "    train_prophet = prophet_model.predict(train[prophet_cols])\n",
    "    train_prophet['Sector'] = sector_name\n",
    "    \n",
    "    xgb_model, scaler, feature_cols = train_enhanced_xgb_model(\n",
    "        train,\n",
    "        train_prophet,\n",
    "        use_early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Test residual ì˜ˆì¸¡\n",
    "    test_features = test[feature_cols].values\n",
    "    test_features_scaled = scaler.transform(test_features)\n",
    "    residual_predicted = xgb_model.predict(test_features_scaled)\n",
    "    \n",
    "    # 3. ê²°ê³¼ ì¡°í•©\n",
    "    result = pd.DataFrame({\n",
    "        'ds': test['ds'].values,\n",
    "        'Sector': sector_name,\n",
    "        'y_actual': test['y'].values,\n",
    "        'yhat_prophet': prophet_forecast['yhat'].values,\n",
    "        'yhat_xgb_resid': residual_predicted,\n",
    "        'yhat_hybrid': prophet_forecast['yhat'].values + residual_predicted,\n",
    "        'used_cps': used_cps\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\" í†µí•© ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7. Rolling Loop ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_YEARS = [2022, 2023, 2024, 2025]\n",
    "sectors = sector_panel['Sector'].unique()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\" Enhanced Rolling Loop v2\")\n",
    "print(f\"Test Years: {TEST_YEARS}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "for year in TEST_YEARS:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Year {year} ì˜ˆì¸¡\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    year_predictions = []\n",
    "    \n",
    "    for i, sector in enumerate(sectors, 1):\n",
    "        print(f\"  [{i}/{len(sectors)}] {sector:25s} \", end=\"\")\n",
    "        \n",
    "        sector_data = sector_panel[sector_panel['Sector'] == sector].copy()\n",
    "        \n",
    "        try:\n",
    "            pred = predict_sector_year_enhanced(sector_data, year, US_HOLIDAYS)\n",
    "            \n",
    "            if pred is not None and len(pred) > 0:\n",
    "                pred['test_year'] = year\n",
    "                pred['train_end_year'] = year - 1\n",
    "                year_predictions.append(pred)\n",
    "                print(f\" ({len(pred)}ì¼, cps={pred['used_cps'].iloc[0]:.3f})\")\n",
    "            else:\n",
    "                print(\"  ë°ì´í„° ë¶€ì¡±\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\" {str(e)[:30]}\")\n",
    "    \n",
    "    if year_predictions:\n",
    "        year_df = pd.concat(year_predictions, ignore_index=True)\n",
    "        all_predictions.append(year_df)\n",
    "        print(f\"\\n   {year}ë…„ ì™„ë£Œ: {len(year_df):,} ë ˆì½”ë“œ\")\n",
    "\n",
    "if all_predictions:\n",
    "    df_predictions = pd.concat(all_predictions, ignore_index=True)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\" ì „ì²´ Rolling Loop ì™„ë£Œ\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"ì´ ë ˆì½”ë“œ: {len(df_predictions):,}\")\n",
    "    print(f\"ì„¹í„° ìˆ˜: {df_predictions['Sector'].nunique()}\")\n",
    "    print(f\"ë…„ë„: {sorted(df_predictions['test_year'].unique())}\")\n",
    "else:\n",
    "    print(\"\\n ì˜ˆì¸¡ ì‹¤íŒ¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8. í‰ê°€\n",
    "\n",
    "(í‰ê°€ í•¨ìˆ˜ëŠ” v1ê³¼ ë™ì¼í•˜ë¯€ë¡œ ìƒëµ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ranking_metrics(df, year, pred_col='yhat_hybrid'):\n",
    "    \"\"\"ë­í‚¹ ê¸°ë°˜ í‰ê°€\"\"\"\n",
    "    year_data = df[df['test_year'] == year].copy()\n",
    "    if len(year_data) == 0:\n",
    "        return None\n",
    "\n",
    "    year_data = year_data.sort_values(['Sector', 'ds'])\n",
    "\n",
    "    def log_return(x):\n",
    "        x = x.dropna()\n",
    "        if len(x) < 2:\n",
    "            return np.nan\n",
    "        return np.exp(x.iloc[-1] - x.iloc[0]) - 1.0\n",
    "\n",
    "    sector_summary = (\n",
    "        year_data.groupby('Sector')\n",
    "        .agg(\n",
    "            Actual_Return=('y_actual', log_return),\n",
    "            Predicted_Return=(pred_col, log_return),\n",
    "        )\n",
    "        .reset_index()\n",
    "        .dropna()\n",
    "    )\n",
    "\n",
    "    sector_summary['Actual_Rank'] = sector_summary['Actual_Return'].rank(ascending=False, method='min')\n",
    "    sector_summary['Predicted_Rank'] = sector_summary['Predicted_Return'].rank(ascending=False, method='min')\n",
    "    sector_summary['Rank_Diff'] = abs(sector_summary['Actual_Rank'] - sector_summary['Predicted_Rank'])\n",
    "\n",
    "    spearman_corr, spearman_pvalue = spearmanr(\n",
    "        sector_summary['Predicted_Return'],\n",
    "        sector_summary['Actual_Return']\n",
    "    )\n",
    "\n",
    "    def top_k_hit_rate(summary_df, k):\n",
    "        k = min(k, len(summary_df))\n",
    "        pred_top_k = set(summary_df.nsmallest(k, 'Predicted_Rank')['Sector'])\n",
    "        actual_top_k = set(summary_df.nsmallest(k, 'Actual_Rank')['Sector'])\n",
    "        return len(pred_top_k & actual_top_k) / k * 100 if k > 0 else np.nan\n",
    "\n",
    "    top3_hit = top_k_hit_rate(sector_summary, 3)\n",
    "    top5_hit = top_k_hit_rate(sector_summary, 5)\n",
    "    close_success_rate = (sector_summary['Rank_Diff'] <= 2).mean() * 100\n",
    "\n",
    "    return {\n",
    "        'Spearman': spearman_corr,\n",
    "        'Spearman_PValue': spearman_pvalue,\n",
    "        'Top3_Hit_Rate': top3_hit,\n",
    "        'Top5_Hit_Rate': top5_hit,\n",
    "        'Close_Success_Rate': close_success_rate,\n",
    "        'Sector_Summary': sector_summary\n",
    "    }\n",
    "\n",
    "print(\" í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 90)\n",
    "print(\" Enhanced v2 í‰ê°€ ê²°ê³¼\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for year in sorted(df_predictions['test_year'].unique()):\n",
    "    print(f\"\\n{'='*90}\")\n",
    "    print(f\"Year {year}\")\n",
    "    print(f\"{'='*90}\")\n",
    "    \n",
    "    ranking_metrics = calculate_ranking_metrics(df_predictions, year, 'yhat_hybrid')\n",
    "    \n",
    "    if ranking_metrics is None:\n",
    "        print(\"  í‰ê°€ ë¶ˆê°€\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n[í•µì‹¬ ë­í‚¹ ì§€í‘œ]\")\n",
    "    print(f\"  Spearman: {ranking_metrics['Spearman']:.4f} (p={ranking_metrics['Spearman_PValue']:.4f})\")\n",
    "    print(f\"  Top-3 Hit: {ranking_metrics['Top3_Hit_Rate']:.1f}%\")\n",
    "    print(f\"  Top-5 Hit: {ranking_metrics['Top5_Hit_Rate']:.1f}%\")\n",
    "    print(f\"  ê·¼ì ‘ ì„±ê³µë¥ : {ranking_metrics['Close_Success_Rate']:.1f}%\")\n",
    "    \n",
    "    sector_summary = ranking_metrics['Sector_Summary'].sort_values('Actual_Rank')\n",
    "    print(f\"\\n[ì„¹í„°ë³„ ìˆœìœ„]\")\n",
    "    print(f\"{'Sector':25s} {'ì‹¤ì œìˆœìœ„':>10s} {'ì˜ˆì¸¡ìˆœìœ„':>10s} {'ì°¨ì´':>8s} {'ì‹¤ì œìˆ˜ìµë¥ ':>12s}\")\n",
    "    print(\"-\" * 70)\n",
    "    for _, row in sector_summary.iterrows():\n",
    "        print(f\"{row['Sector']:25s} {int(row['Actual_Rank']):>10d} {int(row['Predicted_Rank']):>10d} \"\n",
    "              f\"{int(row['Rank_Diff']):>8d} {row['Actual_Return']*100:+11.2f}%\")\n",
    "    \n",
    "    evaluation_results.append({\n",
    "        'Year': year,\n",
    "        'Spearman': ranking_metrics['Spearman'],\n",
    "        'Top3_Hit': ranking_metrics['Top3_Hit_Rate'],\n",
    "        'Top5_Hit': ranking_metrics['Top5_Hit_Rate'],\n",
    "        'Close_Success': ranking_metrics['Close_Success_Rate']\n",
    "    })\n",
    "\n",
    "df_eval_summary = pd.DataFrame(evaluation_results)\n",
    "\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\" ì „ì²´ í‰ê·  ì„±ëŠ¥ (v2)\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"  í‰ê·  Spearman: {df_eval_summary['Spearman'].dropna().mean():.4f}\")\n",
    "print(f\"  í‰ê·  Top-3 Hit: {df_eval_summary['Top3_Hit'].mean():.1f}%\")\n",
    "print(f\"  í‰ê·  Top-5 Hit: {df_eval_summary['Top5_Hit'].mean():.1f}%\")\n",
    "print(f\"  í‰ê·  ê·¼ì ‘ ì„±ê³µë¥ : {df_eval_summary['Close_Success'].mean():.1f}%\")\n",
    "\n",
    "print(\"\\nğŸ¯ íŠ¹íˆ 2023ë…„ ì„±ëŠ¥ì„ í™•ì¸í•˜ì„¸ìš”!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9. ê²°ê³¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ì €ì¥\n",
    "df_predictions.to_csv(OUTPUT_DIR / 'enhanced_v2_predictions.csv', index=False)\n",
    "df_eval_summary.to_csv(OUTPUT_DIR / 'enhanced_v2_evaluation.csv', index=False)\n",
    "\n",
    "print(\"ê²°ê³¼ ì €ì¥ ì™„ë£Œ\")\n",
    "print(f\"  - {OUTPUT_DIR / 'enhanced_v2_predictions.csv'}\")\n",
    "print(f\"  - {OUTPUT_DIR / 'enhanced_v2_evaluation.csv'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" Enhanced Prophet v2 ë¶„ì„ ì™„ë£Œ\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
