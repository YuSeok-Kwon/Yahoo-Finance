{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb838cd",
   "metadata": {},
   "source": [
    "# 03. Data Update & Quality Assurance (v2)\n",
    "## 목표\n",
    "1. 전체 기간 데이터를 수정주가(Adjusted Close) 기준으로 재수집\n",
    "2. 티커 메타데이터(섹터/산업) 최신화\n",
    "3. 품질 검증 및 데이터 정제\n",
    "4. 파생변수 계산 및 저장\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596791b3",
   "metadata": {},
   "source": [
    "## Section 1. 환경 설정 & 티커 목록 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4699884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 시각화 설정\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0448e0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  작업 디렉토리: /Users/yu_seok/Documents/workspace/nbCamp/Project/Yahoo Finance\n",
      "  데이터 디렉토리: /Users/yu_seok/Documents/workspace/nbCamp/Project/Yahoo Finance/Data_set\n"
     ]
    }
   ],
   "source": [
    "# 경로 설정\n",
    "PROJECT_ROOT = Path('.').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / 'Data_set'\n",
    "QA_DIR = DATA_DIR / 'QA'\n",
    "LOG_DIR = DATA_DIR / 'Logs'\n",
    "TEMP_DIR = DATA_DIR / 'temp_batches'\n",
    "\n",
    "# 디렉토리 생성\n",
    "for directory in [DATA_DIR, QA_DIR, LOG_DIR, TEMP_DIR]:\n",
    "    directory.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"  작업 디렉토리: {PROJECT_ROOT}\")\n",
    "print(f\"  데이터 디렉토리: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "logging_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 13:09:39,482 - INFO - ============================================================\n",
      "2026-01-13 13:09:39,482 - INFO - 데이터 업데이트 프로세스 시작\n",
      "2026-01-13 13:09:39,483 - INFO - ============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  로그 파일: /Users/yu_seok/Documents/workspace/nbCamp/Project/Yahoo Finance/Data_set/Logs/data_update_20260113_130939.log\n"
     ]
    }
   ],
   "source": [
    "# 로깅 설정\n",
    "log_file = LOG_DIR / f'data_update_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file, encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"데이터 업데이트 프로세스 시작\")\n",
    "logger.info(\"=\" * 60)\n",
    "print(f\"  로그 파일: {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "resume_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 13:09:39,490 - INFO - 새로운 작업 시작\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 새로운 작업을 시작합니다.\n"
     ]
    }
   ],
   "source": [
    "# 이전 작업 재개 기능\n",
    "def check_previous_work():\n",
    "    \"\"\"이전 작업 상태 확인 및 재개 옵션 제공\"\"\"\n",
    "    batch_files = list(TEMP_DIR.glob('batch_*.csv'))\n",
    "    \n",
    "    if batch_files:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"  이전 작업 발견\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"발견된 배치 파일: {len(batch_files)}개\")\n",
    "        print(f\"위치: {TEMP_DIR}\")\n",
    "        \n",
    "        # 자동으로 이어서 진행\n",
    "        response = input(\"\\n이전 작업을 이어서 진행하시겠습니까? (y/n) [y]: \").strip().lower()\n",
    "        \n",
    "        if response in ['', 'y', 'yes']:\n",
    "            logger.info(f\"이전 작업 재개 - {len(batch_files)}개 배치 발견\")\n",
    "            print(\" 이전 작업을 이어서 진행합니다.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"\\n기존 임시 파일을 삭제하고 새로 시작합니다\")\n",
    "            shutil.rmtree(TEMP_DIR)\n",
    "            TEMP_DIR.mkdir()\n",
    "            logger.info(\"임시 파일 삭제 후 새로 시작\")\n",
    "            print(\" 새로운 작업을 시작합니다.\")\n",
    "            return False\n",
    "    else:\n",
    "        logger.info(\"새로운 작업 시작\")\n",
    "        print(\" 새로운 작업을 시작합니다.\")\n",
    "        return False\n",
    "\n",
    "RESUME_MODE = check_previous_work()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cfa40180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "티커 목록 로드\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 13:09:41,700 - INFO - 기존 파일 로드 완료: /Users/yu_seok/Documents/workspace/nbCamp/Project/Yahoo Finance/Data_set/stock_daily_master.csv\n",
      "2026-01-13 13:09:41,831 - INFO - 티커 목록 로드 완료: 481개\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  총 티커 수: 481\n",
      "  샘플 (처음 20개): ['A', 'AAPL', 'ABBV', 'ABEV', 'ABNB', 'ABT', 'ACGL', 'ACN', 'ADBE', 'ADI', 'ADM', 'ADP', 'ADSK', 'AEE', 'AEM', 'AEP', 'AFL', 'AIG', 'AJG', 'ALC']\n"
     ]
    }
   ],
   "source": [
    "# 티커 목록 로드\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"티커 목록 로드\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "master_file = DATA_DIR / 'stock_daily_master.csv'\n",
    "\n",
    "if not master_file.exists():\n",
    "    logger.error(f\"파일을 찾을 수 없습니다: {master_file}\")\n",
    "    raise FileNotFoundError(f\"파일이 존재하지 않습니다: {master_file}\")\n",
    "\n",
    "try:\n",
    "    df_old = pd.read_csv(master_file)\n",
    "    logger.info(f\"기존 파일 로드 완료: {master_file}\")\n",
    "    \n",
    "    tickers = (\n",
    "        df_old['Company']\n",
    "        .dropna()\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .loc[lambda s: s.ne('') & s.ne('nan')]\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "    tickers = sorted(tickers)\n",
    "    \n",
    "    print(f\"  총 티커 수: {len(tickers)}\")\n",
    "    print(f\"  샘플 (처음 20개): {tickers[:20]}\")\n",
    "    logger.info(f\"티커 목록 로드 완료: {len(tickers)}개\")\n",
    "    \n",
    "    # 메모리 해제\n",
    "    del df_old\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"티커 목록 로드 실패: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059d761",
   "metadata": {},
   "source": [
    "## Section 2. 전체 기간 데이터 재수집 (수정주가 반영)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19755a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_adjusted_price_data(ticker_list, start_date, end_date, batch_size=10, max_retries=3):\n",
    "    \"\"\"\n",
    "    yfinance 데이터 수집 함수\n",
    "    \n",
    "    Args:\n",
    "        ticker_list: 티커 목록\n",
    "        start_date: 시작일 (YYYY-MM-DD)\n",
    "        end_date: 종료일 (YYYY-MM-DD)\n",
    "        batch_size: 배치 크기 (기본값: 10)\n",
    "        max_retries: 최대 재시도 횟수 (기본값: 3)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: 수정주가 반영된 OHLCV 데이터\n",
    "        list: 실패한 티커 목록\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    failed_tickers = []\n",
    "    \n",
    "    # 배치 분할\n",
    "    chunks = [ticker_list[i:i + batch_size] for i in range(0, len(ticker_list), batch_size)]\n",
    "    logger.info(f\"총 {len(chunks)}개 배치로 분할 (배치 크기: {batch_size})\")\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(tqdm(chunks, desc=\" 데이터 수집 중\")):\n",
    "        # 이미 저장된 배치가 있으면 건너뛰기\n",
    "        temp_file = TEMP_DIR / f'batch_{chunk_idx:04d}.csv'\n",
    "        if temp_file.exists():\n",
    "            try:\n",
    "                df_batch = pd.read_csv(temp_file, parse_dates=['Date'])\n",
    "                all_data.append(df_batch)\n",
    "                logger.info(f\"배치 {chunk_idx}/{len(chunks)} - 캐시에서 로드됨 ({len(df_batch)} 행)\")\n",
    "                print(f\"    배치 {chunk_idx} 캐시에서 로드\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"배치 {chunk_idx} 캐시 로드 실패: {str(e)}, 재수집 시도\")\n",
    "        \n",
    "        tickers_str = ' '.join(chunk)\n",
    "        batch_success = False\n",
    "        \n",
    "        # Exponential backoff 재시도\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # 배치 다운로드\n",
    "                data = yf.download(\n",
    "                    tickers_str,\n",
    "                    start=start_date,\n",
    "                    end=end_date,\n",
    "                    group_by='ticker',\n",
    "                    auto_adjust=True,\n",
    "                    progress=False,\n",
    "                    threads=False  # Rate limit 방지를 위해 단일 스레드 사용\n",
    "                )\n",
    "                \n",
    "                # 데이터가 비어있는지 확인\n",
    "                if data.empty:\n",
    "                    logger.warning(f\"배치 {chunk_idx} - 데이터 없음\")\n",
    "                    failed_tickers.extend(chunk)\n",
    "                    break\n",
    "                \n",
    "                # 데이터 처리\n",
    "                batch_data = []\n",
    "                \n",
    "                if len(chunk) == 1:\n",
    "                    # 단일 티커 처리\n",
    "                    symbol = chunk[0]\n",
    "                    if not data.empty:\n",
    "                        df_ticker = data.copy().reset_index()\n",
    "                        \n",
    "                        # 컬럼명 정규화\n",
    "                        if 'index' in df_ticker.columns and 'Date' not in df_ticker.columns:\n",
    "                            df_ticker = df_ticker.rename(columns={'index': 'Date'})\n",
    "                        \n",
    "                        df_ticker['Company'] = symbol\n",
    "                        \n",
    "                        # Close 값이 있는 행만 유지\n",
    "                        if 'Close' in df_ticker.columns:\n",
    "                            df_ticker = df_ticker.dropna(subset=['Close'])\n",
    "                            if not df_ticker.empty:\n",
    "                                batch_data.append(df_ticker)\n",
    "                            else:\n",
    "                                failed_tickers.append(symbol)\n",
    "                        else:\n",
    "                            failed_tickers.append(symbol)\n",
    "                    else:\n",
    "                        failed_tickers.append(symbol)\n",
    "                else:\n",
    "                    # 멀티 티커 처리\n",
    "                    for symbol in chunk:\n",
    "                        try:\n",
    "                            # 티커 데이터 추출\n",
    "                            if hasattr(data.columns, 'levels'):\n",
    "                                # MultiIndex인 경우\n",
    "                                if symbol in data.columns.get_level_values(0):\n",
    "                                    df_ticker = data[symbol].copy()\n",
    "                                else:\n",
    "                                    failed_tickers.append(symbol)\n",
    "                                    continue\n",
    "                            else:\n",
    "                                # 단순 Index인 경우\n",
    "                                df_ticker = data.copy()\n",
    "                            \n",
    "                            # Date를 컬럼으로 변환\n",
    "                            df_ticker = df_ticker.reset_index()\n",
    "                            \n",
    "                            # 컬럼명 정규화\n",
    "                            if 'index' in df_ticker.columns and 'Date' not in df_ticker.columns:\n",
    "                                df_ticker = df_ticker.rename(columns={'index': 'Date'})\n",
    "                            \n",
    "                            df_ticker['Company'] = symbol\n",
    "                            \n",
    "                            # Close 값이 있는 행만 유지\n",
    "                            if 'Close' in df_ticker.columns:\n",
    "                                df_ticker = df_ticker.dropna(subset=['Close'])\n",
    "                                if not df_ticker.empty:\n",
    "                                    batch_data.append(df_ticker)\n",
    "                                else:\n",
    "                                    failed_tickers.append(symbol)\n",
    "                            else:\n",
    "                                failed_tickers.append(symbol)\n",
    "                        \n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"티커 {symbol} 처리 실패: {str(e)}\")\n",
    "                            failed_tickers.append(symbol)\n",
    "                \n",
    "                # 배치 데이터 저장\n",
    "                if batch_data:\n",
    "                    df_batch = pd.concat(batch_data, ignore_index=True)\n",
    "                    \n",
    "                    # Date 컬럼 처리\n",
    "                    if 'Date' in df_batch.columns:\n",
    "                        df_batch['Date'] = pd.to_datetime(df_batch['Date'])\n",
    "                        if df_batch['Date'].dt.tz is not None:\n",
    "                            df_batch['Date'] = df_batch['Date'].dt.tz_localize(None)\n",
    "                    \n",
    "                    # 임시 파일로 저장\n",
    "                    df_batch.to_csv(temp_file, index=False)\n",
    "                    all_data.append(df_batch)\n",
    "                    \n",
    "                    logger.info(f\"배치 {chunk_idx}/{len(chunks)} 성공 - {len(df_batch)} 행 저장\")\n",
    "                    print(f\"     배치 {chunk_idx} 완료 ({len(df_batch)} 행)\")\n",
    "                    batch_success = True\n",
    "                else:\n",
    "                    logger.warning(f\"배치 {chunk_idx} - 처리된 데이터 없음\")\n",
    "                \n",
    "                break  # 성공 시 재시도 루프 종료\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "                \n",
    "                # Rate Limit 에러 감지\n",
    "                if 'Rate' in error_str or '429' in error_str or 'Too Many Requests' in error_str:\n",
    "                    wait_time = (2 ** attempt) * 2  # 2초, 4초, 8초\n",
    "                    logger.warning(f\"배치 {chunk_idx} - {wait_time}초 대기 (시도 {attempt+1}/{max_retries})\")\n",
    "                    print(f\"   Rate Limit 감지, {wait_time}초 대기 중\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    logger.error(f\"배치 {chunk_idx} 에러: {error_str}\")\n",
    "                    if attempt == max_retries - 1:\n",
    "                        failed_tickers.extend(chunk)\n",
    "                    break\n",
    "        \n",
    "        if not batch_success and attempt == max_retries - 1:\n",
    "            logger.error(f\"배치 {chunk_idx} 최종 실패\")\n",
    "        \n",
    "        # 배치 간 대기 (Rate limit 방지)\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # 데이터 병합\n",
    "    if all_data:\n",
    "        logger.info(f\"총 {len(all_data)}개 배치 데이터 병합 중\")\n",
    "        df_combined = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        # Date 컬럼 최종 처리\n",
    "        if 'Date' in df_combined.columns:\n",
    "            df_combined['Date'] = pd.to_datetime(df_combined['Date'])\n",
    "            if df_combined['Date'].dt.tz is not None:\n",
    "                df_combined['Date'] = df_combined['Date'].dt.tz_localize(None)\n",
    "        \n",
    "        logger.info(f\"최종 데이터: {df_combined.shape}\")\n",
    "        logger.info(f\"컬럼: {df_combined.columns.tolist()}\")\n",
    "        \n",
    "        return df_combined, failed_tickers\n",
    "    else:\n",
    "        logger.error(\"수집된 데이터가 없습니다\")\n",
    "        return pd.DataFrame(), failed_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ebc1484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 13:09:41,869 - INFO - 데이터 수집 시작 - 기간: 2018-11-29 ~ 2026-01-13, 티커 수: 481\n",
      "2026-01-13 13:09:41,870 - INFO - 총 49개 배치로 분할 (배치 크기: 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "전체 기간 데이터 재수집 (수정주가 반영)\n",
      "============================================================\n",
      "기간: 2018-11-29 ~ 2026-01-13\n",
      "티커 수: 481\n",
      "예상 소요 시간: ~4.0분\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:   0%|          | 0/49 [00:00<?, ?it/s]2026-01-13 13:09:44,936 - INFO - 배치 0/49 성공 - 17369 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 0 완료 (17369 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:   2%|▏         | 1/49 [00:05<04:02,  5.06s/it]2026-01-13 13:09:49,917 - INFO - 배치 1/49 성공 - 17792 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 1 완료 (17792 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:   4%|▍         | 2/49 [00:10<03:55,  5.01s/it]2026-01-13 13:09:54,406 - INFO - 배치 2/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 2 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:   6%|▌         | 3/49 [00:14<03:39,  4.77s/it]2026-01-13 13:09:59,009 - INFO - 배치 3/49 성공 - 16676 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 3 완료 (16676 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:   8%|▊         | 4/49 [00:19<03:31,  4.71s/it]2026-01-13 13:10:04,097 - INFO - 배치 4/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 4 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  10%|█         | 5/49 [00:24<03:33,  4.84s/it]2026-01-13 13:10:08,700 - INFO - 배치 5/49 성공 - 17452 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 5 완료 (17452 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  12%|█▏        | 6/49 [00:28<03:24,  4.76s/it]2026-01-13 13:10:13,581 - INFO - 배치 6/49 성공 - 17664 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 6 완료 (17664 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  14%|█▍        | 7/49 [00:33<03:21,  4.80s/it]2026-01-13 13:10:18,237 - INFO - 배치 7/49 성공 - 17554 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 7 완료 (17554 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  16%|█▋        | 8/49 [00:38<03:14,  4.76s/it]2026-01-13 13:10:22,682 - INFO - 배치 8/49 성공 - 17091 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 8 완료 (17091 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  18%|█▊        | 9/49 [00:42<03:06,  4.66s/it]2026-01-13 13:10:27,488 - INFO - 배치 9/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 9 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  20%|██        | 10/49 [00:47<03:03,  4.70s/it]2026-01-13 13:10:32,395 - INFO - 배치 10/49 성공 - 16713 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 10 완료 (16713 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  22%|██▏       | 11/49 [00:52<03:01,  4.77s/it]2026-01-13 13:10:36,939 - INFO - 배치 11/49 성공 - 17628 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 11 완료 (17628 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  24%|██▍       | 12/49 [00:57<02:53,  4.70s/it]2026-01-13 13:10:41,840 - INFO - 배치 12/49 성공 - 17169 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 12 완료 (17169 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  27%|██▋       | 13/49 [01:01<02:51,  4.76s/it]2026-01-13 13:10:46,599 - INFO - 배치 13/49 성공 - 17806 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 13 완료 (17806 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  29%|██▊       | 14/49 [01:06<02:46,  4.76s/it]2026-01-13 13:10:51,487 - INFO - 배치 14/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 14 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  31%|███       | 15/49 [01:11<02:43,  4.80s/it]2026-01-13 13:10:56,311 - INFO - 배치 15/49 성공 - 17869 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 15 완료 (17869 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  33%|███▎      | 16/49 [01:16<02:38,  4.81s/it]2026-01-13 13:11:01,353 - INFO - 배치 16/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 16 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  35%|███▍      | 17/49 [01:21<02:36,  4.88s/it]2026-01-13 13:11:06,162 - INFO - 배치 17/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 17 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  37%|███▋      | 18/49 [01:26<02:30,  4.86s/it]2026-01-13 13:11:10,793 - INFO - 배치 18/49 성공 - 16129 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 18 완료 (16129 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  39%|███▉      | 19/49 [01:30<02:23,  4.79s/it]2026-01-13 13:11:15,350 - INFO - 배치 19/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 19 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  41%|████      | 20/49 [01:35<02:16,  4.72s/it]2026-01-13 13:11:20,021 - INFO - 배치 20/49 성공 - 16963 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 20 완료 (16963 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  43%|████▎     | 21/49 [01:40<02:11,  4.70s/it]2026-01-13 13:11:24,820 - INFO - 배치 21/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 21 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  45%|████▍     | 22/49 [01:44<02:07,  4.73s/it]2026-01-13 13:11:29,481 - INFO - 배치 22/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 22 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  47%|████▋     | 23/49 [01:49<02:02,  4.71s/it]2026-01-13 13:11:34,234 - INFO - 배치 23/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 23 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  49%|████▉     | 24/49 [01:54<01:58,  4.72s/it]2026-01-13 13:11:39,161 - INFO - 배치 24/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 24 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  51%|█████     | 25/49 [01:59<01:54,  4.78s/it]2026-01-13 13:11:44,027 - INFO - 배치 25/49 성공 - 16349 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 25 완료 (16349 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  53%|█████▎    | 26/49 [02:04<01:50,  4.81s/it]2026-01-13 13:11:48,851 - INFO - 배치 26/49 성공 - 16897 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 26 완료 (16897 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  55%|█████▌    | 27/49 [02:08<01:45,  4.81s/it]2026-01-13 13:11:53,393 - INFO - 배치 27/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 27 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  57%|█████▋    | 28/49 [02:13<01:39,  4.73s/it]2026-01-13 13:11:58,211 - INFO - 배치 28/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 28 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  59%|█████▉    | 29/49 [02:18<01:35,  4.76s/it]2026-01-13 13:12:02,791 - INFO - 배치 29/49 성공 - 17875 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 29 완료 (17875 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  61%|██████    | 30/49 [02:22<01:29,  4.71s/it]2026-01-13 13:12:07,508 - INFO - 배치 30/49 성공 - 17683 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 30 완료 (17683 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  63%|██████▎   | 31/49 [02:27<01:24,  4.71s/it]2026-01-13 13:12:12,201 - INFO - 배치 31/49 성공 - 17118 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 31 완료 (17118 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  65%|██████▌   | 32/49 [02:32<01:19,  4.70s/it]2026-01-13 13:12:17,003 - INFO - 배치 32/49 성공 - 17554 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 32 완료 (17554 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  67%|██████▋   | 33/49 [02:37<01:15,  4.73s/it]2026-01-13 13:12:21,863 - INFO - 배치 33/49 성공 - 17840 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 33 완료 (17840 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  69%|██████▉   | 34/49 [02:41<01:11,  4.77s/it]2026-01-13 13:12:26,514 - INFO - 배치 34/49 성공 - 17324 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 34 완료 (17324 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  71%|███████▏  | 35/49 [02:46<01:06,  4.74s/it]2026-01-13 13:12:31,207 - INFO - 배치 35/49 성공 - 17309 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 35 완료 (17309 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  73%|███████▎  | 36/49 [02:51<01:01,  4.72s/it]2026-01-13 13:12:36,051 - INFO - 배치 36/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 36 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  76%|███████▌  | 37/49 [02:56<00:57,  4.76s/it]2026-01-13 13:12:40,728 - INFO - 배치 37/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 37 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  78%|███████▊  | 38/49 [03:00<00:52,  4.73s/it]2026-01-13 13:12:45,199 - INFO - 배치 38/49 성공 - 17429 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 38 완료 (17429 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  80%|███████▉  | 39/49 [03:05<00:46,  4.65s/it]2026-01-13 13:12:49,974 - INFO - 배치 39/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 39 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  82%|████████▏ | 40/49 [03:10<00:42,  4.69s/it]2026-01-13 13:12:54,665 - INFO - 배치 40/49 성공 - 17310 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 40 완료 (17310 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  84%|████████▎ | 41/49 [03:14<00:37,  4.69s/it]2026-01-13 13:12:59,647 - INFO - 배치 41/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 41 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  86%|████████▌ | 42/49 [03:19<00:33,  4.78s/it]2026-01-13 13:13:04,225 - INFO - 배치 42/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 42 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  88%|████████▊ | 43/49 [03:24<00:28,  4.72s/it]2026-01-13 13:13:09,130 - INFO - 배치 43/49 성공 - 17685 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 43 완료 (17685 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  90%|████████▉ | 44/49 [03:29<00:23,  4.78s/it]2026-01-13 13:13:13,631 - INFO - 배치 44/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 44 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  92%|█████████▏| 45/49 [03:33<00:18,  4.69s/it]2026-01-13 13:13:18,050 - INFO - 배치 45/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 45 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  94%|█████████▍| 46/49 [03:38<00:13,  4.61s/it]2026-01-13 13:13:22,786 - INFO - 배치 46/49 성공 - 17880 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 46 완료 (17880 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  96%|█████████▌| 47/49 [03:42<00:09,  4.65s/it]2026-01-13 13:13:27,539 - INFO - 배치 47/49 성공 - 17785 행 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     배치 47 완료 (17785 행)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 데이터 수집 중:  98%|█████████▊| 48/49 [03:47<00:04,  4.68s/it]2026-01-13 13:13:29,889 - WARNING - 배치 48 - 처리된 데이터 없음\n",
      " 데이터 수집 중: 100%|██████████| 49/49 [03:50<00:00,  4.69s/it]\n",
      "2026-01-13 13:13:31,896 - INFO - 총 48개 배치 데이터 병합 중...\n",
      "2026-01-13 13:13:31,926 - INFO - 최종 데이터: (843513, 7)\n",
      "2026-01-13 13:13:31,927 - INFO - 컬럼: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Company']\n",
      "2026-01-13 13:13:31,949 - WARNING - 실패 티커: 1개 - ['ZS']\n",
      "2026-01-13 13:13:31,983 - INFO - 수집 성공 - Shape: (843513, 7), 기업 수: 480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "수집 결과\n",
      "============================================================\n",
      "  성공: 480개\n",
      " 실패: 1개\n",
      "\n",
      "실패 티커 (처음 20개): ['ZS']\n",
      "\n",
      "  데이터 수집 성공\n",
      "  Shape: (843513, 7)\n",
      "  컬럼: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Company']\n",
      "  기간: 2018-11-29 00:00:00 ~ 2026-01-12 00:00:00\n",
      "  기업 수: 480\n",
      "  - Date:  \n",
      "  - Company:  \n",
      "  - Close:  \n"
     ]
    }
   ],
   "source": [
    "# 전체 기간 재수집 실행\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"전체 기간 데이터 재수집 (수정주가 반영)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 수집 기간 설정\n",
    "start_date = '2018-11-29'\n",
    "end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"기간: {start_date} ~ {end_date}\")\n",
    "print(f\"티커 수: {len(tickers)}\")\n",
    "print(f\"예상 소요 시간: ~{len(tickers) * 0.5 / 60:.1f}분\\n\")\n",
    "\n",
    "logger.info(f\"데이터 수집 시작 - 기간: {start_date} ~ {end_date}, 티커 수: {len(tickers)}\")\n",
    "\n",
    "# 재수집 실행\n",
    "df_raw, failed = fetch_adjusted_price_data(\n",
    "    tickers, \n",
    "    start_date, \n",
    "    end_date, \n",
    "    batch_size=10,\n",
    "    max_retries=3\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"수집 결과\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  성공: {len(tickers) - len(failed)}개\")\n",
    "print(f\" 실패: {len(failed)}개\")\n",
    "\n",
    "if failed:\n",
    "    print(f\"\\n실패 티커 (처음 20개): {failed[:20]}\")\n",
    "    logger.warning(f\"실패 티커: {len(failed)}개 - {failed[:50]}\")\n",
    "\n",
    "# 결과 확인\n",
    "if not df_raw.empty:\n",
    "    print(f\"\\n  데이터 수집 성공\")\n",
    "    print(f\"  Shape: {df_raw.shape}\")\n",
    "    print(f\"  컬럼: {df_raw.columns.tolist()}\")\n",
    "    print(f\"  기간: {df_raw['Date'].min()} ~ {df_raw['Date'].max()}\")\n",
    "    print(f\"  기업 수: {df_raw['Company'].nunique()}\")\n",
    "    \n",
    "    # 필수 컬럼 확인\n",
    "    required_cols = ['Date', 'Company', 'Close']\n",
    "    for col in required_cols:\n",
    "        print(f\"  - {col}: {' ' if col in df_raw.columns else '   '}\")\n",
    "    \n",
    "    logger.info(f\"수집 성공 - Shape: {df_raw.shape}, 기업 수: {df_raw['Company'].nunique()}\")\n",
    "else:\n",
    "    print(\"\\n    데이터가 비어있습니다.\")\n",
    "    logger.error(\"수집된 데이터가 없습니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "retry_failed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 13:13:31,994 - INFO - 실패 티커 재시도 시작: 1개\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "실패한 티커 재수집 시도\n",
      "============================================================\n",
      "재시도할 티커 수: 1\n",
      "더 작은 배치 크기(5)로 재시도 중\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 13:13:36,998 - INFO - 총 1개 배치로 분할 (배치 크기: 5)\n",
      " 데이터 수집 중:   0%|          | 0/1 [00:00<?, ?it/s]2026-01-13 13:13:37,023 - INFO - 배치 0/1 - 캐시에서 로드됨 (17369 행)\n",
      " 데이터 수집 중: 100%|██████████| 1/1 [00:00<00:00, 46.59it/s]\n",
      "2026-01-13 13:13:37,023 - INFO - 총 1개 배치 데이터 병합 중...\n",
      "2026-01-13 13:13:37,029 - INFO - 최종 데이터: (17369, 7)\n",
      "2026-01-13 13:13:37,029 - INFO - 컬럼: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Company']\n",
      "2026-01-13 13:13:37,041 - INFO - 재수집 성공: 1개\n",
      "2026-01-13 13:13:37,041 - INFO - 최종 실패 티커: 0개\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    배치 0 캐시에서 로드\n",
      "\n",
      "  재수집 성공: 1개\n",
      "최종 실패: 0개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 13:13:39,530 - INFO - 백업 저장: /Users/yu_seok/Documents/workspace/nbCamp/Project/Yahoo Finance/Data_set/raw_data_backup_20260113_131337.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  백업 저장 완료: raw_data_backup_20260113_131337.csv\n"
     ]
    }
   ],
   "source": [
    "# 실패한 티커 재시도\n",
    "if failed and len(failed) > 0:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"실패한 티커 재수집 시도\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"재시도할 티커 수: {len(failed)}\")\n",
    "    print(f\"더 작은 배치 크기(5)로 재시도 중\\n\")\n",
    "    \n",
    "    logger.info(f\"실패 티커 재시도 시작: {len(failed)}개\")\n",
    "    \n",
    "    # 5초 대기 후 재시도\n",
    "    time.sleep(5)\n",
    "    \n",
    "    df_retry, failed_final = fetch_adjusted_price_data(\n",
    "        failed, \n",
    "        start_date, \n",
    "        end_date, \n",
    "        batch_size=5,   # 더 작은 배치\n",
    "        max_retries=5   # 더 많은 재시도\n",
    "    )\n",
    "    \n",
    "    # --- 재시도 병합 가드 (캐시 중복 방지) ---\n",
    "    def _normalize_tickers(tickers):\n",
    "        \"\"\"티커 목록 정규화 (대문자, 공백 제거)\"\"\"\n",
    "        if tickers is None:\n",
    "            return []\n",
    "        return [str(t).strip().upper() for t in tickers if str(t).strip()]\n",
    "    \n",
    "    if df_retry is None or df_retry.empty:\n",
    "        logger.info(\"재수집 결과가 비어 있음: df_retry empty -> 병합 생략\")\n",
    "        print(f\"\\n    재수집 실패\")\n",
    "    else:\n",
    "        # 티커 목록 정규화\n",
    "        failed_norm = _normalize_tickers(failed)\n",
    "        failed_final_norm = _normalize_tickers(failed_final)\n",
    "        \n",
    "        retry_requested = set(failed_norm)\n",
    "        retry_failed = set(failed_final_norm)\n",
    "        retry_succeeded = retry_requested - retry_failed\n",
    "        \n",
    "        logger.info(\n",
    "            f\"재수집 요약 - 요청:{len(retry_requested)} 성공:{len(retry_succeeded)} 실패:{len(retry_failed)}\"\n",
    "        )\n",
    "        \n",
    "        if not retry_succeeded:\n",
    "            logger.warning(\n",
    "                f\"재수집 성공 티커가 없음 -> df_retry 병합 생략 (요청={sorted(retry_requested)}, 실패={sorted(retry_failed)})\"\n",
    "            )\n",
    "            print(f\"\\n    재수집 실패 (모든 티커 실패)\")\n",
    "        else:\n",
    "            # 티커 컬럼 식별\n",
    "            candidate_cols = [\"Company\", \"Ticker\", \"Symbol\"]\n",
    "            ticker_col = next((c for c in candidate_cols if c in df_retry.columns), None)\n",
    "            \n",
    "            if ticker_col is None:\n",
    "                raise KeyError(\n",
    "                    f\"df_retry에 티커 컬럼이 없음. 기대 컬럼 후보={candidate_cols}, 실제 컬럼={list(df_retry.columns)}\"\n",
    "                )\n",
    "            \n",
    "            # 티커 컬럼 정규화 (복사본에서 작업)\n",
    "            df_retry_work = df_retry.copy()\n",
    "            df_retry_work[ticker_col] = df_retry_work[ticker_col].astype(str).str.strip().str.upper()\n",
    "            \n",
    "            # 예상치 못한 티커 경고 (캐시 오염 감지)\n",
    "            retry_returned_tickers = set(df_retry_work[ticker_col].dropna().unique())\n",
    "            unexpected = retry_returned_tickers - retry_succeeded\n",
    "            if unexpected:\n",
    "                logger.warning(\n",
    "                    f\"재수집 df에 예상치 못한 티커가 포함됨 (캐시 오염/배치 재사용 가능). \"\n",
    "                    f\"예상={len(retry_succeeded)}개, 실제={len(retry_returned_tickers)}개, \"\n",
    "                    f\"unexpected(sample)={sorted(list(unexpected))[:20]}\"\n",
    "                )\n",
    "            \n",
    "            # 실제 성공한 티커만 필터링\n",
    "            df_retry_filtered = df_retry_work[df_retry_work[ticker_col].isin(retry_succeeded)]\n",
    "            \n",
    "            if df_retry_filtered.empty:\n",
    "                logger.warning(\n",
    "                    f\"재수집 성공 티커({sorted(list(retry_succeeded))})에 해당하는 행이 df_retry에 없음 -> 병합 생략\"\n",
    "                )\n",
    "                print(f\"\\n    재수집 데이터 없음 (필터링 후 빈 결과)\")\n",
    "            else:\n",
    "                pre_rows = len(df_raw)\n",
    "                add_rows = len(df_retry_filtered)\n",
    "                \n",
    "                # 병합 실행\n",
    "                df_raw = pd.concat([df_raw, df_retry_filtered], ignore_index=True)\n",
    "                \n",
    "                logger.info(\n",
    "                    f\"재수집 데이터 병합 완료: +{add_rows} 행 (이전={pre_rows}, 이후={len(df_raw)})\"\n",
    "                )\n",
    "                print(f\"\\n  재수집 성공: {len(retry_succeeded)}개 티커, +{add_rows:,} 행\")\n",
    "                \n",
    "                # 중복 제거 가드 (안전장치)\n",
    "                dedupe_keys = []\n",
    "                if ticker_col in df_raw.columns:\n",
    "                    dedupe_keys.append(ticker_col)\n",
    "                if \"Date\" in df_raw.columns:\n",
    "                    dedupe_keys.append(\"Date\")\n",
    "                \n",
    "                if len(dedupe_keys) >= 2:\n",
    "                    before = len(df_raw)\n",
    "                    df_raw = df_raw.drop_duplicates(subset=dedupe_keys, keep=\"last\").reset_index(drop=True)\n",
    "                    removed = before - len(df_raw)\n",
    "                    if removed:\n",
    "                        logger.warning(f\"중복 제거 수행: keys={dedupe_keys}, 제거={removed} 행\")\n",
    "                        print(f\"    중복 제거: {removed:,} 행 제거됨\")\n",
    "                    else:\n",
    "                        logger.info(f\"중복 없음 확인: keys={dedupe_keys}\")\n",
    "                else:\n",
    "                    logger.info(f\"중복 제거 스킵: dedupe key 부족 (columns={dedupe_keys})\")\n",
    "    \n",
    "    failed = failed_final\n",
    "    print(f\"최종 실패: {len(failed)}개\")\n",
    "    logger.info(f\"최종 실패 티커: {len(failed)}개\")\n",
    "\n",
    "# 최종 결과 백업 저장\n",
    "if not df_raw.empty:\n",
    "    backup_path = DATA_DIR / f'raw_data_backup_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "    df_raw.to_csv(backup_path, index=False)\n",
    "    print(f\"\\n  백업 저장 완료: {backup_path.name}\")\n",
    "    logger.info(f\"백업 저장: {backup_path}\")\n",
    "    \n",
    "    # 실패 티커 목록 저장\n",
    "    if failed:\n",
    "        failed_file = LOG_DIR / f'failed_tickers_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt'\n",
    "        with open(failed_file, 'w') as f:\n",
    "            f.write('\\n'.join(failed))\n",
    "        print(f\"  실패 티커 목록 저장: {failed_file.name}\")\n",
    "        logger.info(f\"실패 티커 목록 저장: {failed_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc30b75",
   "metadata": {},
   "source": [
    "## Section 3. 티커 메타데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "section3_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 13:13:39,536 - INFO - Section 3 시작 - 메타데이터 수집\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Section 3: 티커 메타데이터 수집\n",
      "============================================================\n",
      "  티커 목록 확인: 481개\n"
     ]
    }
   ],
   "source": [
    "# Section 3 전제 조건 체크\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Section 3: 티커 메타데이터 수집\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 티커 목록 확인\n",
    "if 'tickers' not in locals() or len(tickers) == 0:\n",
    "    logger.error(\"티커 목록이 없습니다\")\n",
    "    raise SystemExit(\"  티커 목록이 없습니다. Section 1을 먼저 실행하세요.\")\n",
    "\n",
    "print(f\"  티커 목록 확인: {len(tickers)}개\")\n",
    "logger.info(\"Section 3 시작 - 메타데이터 수집\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "591a8cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 티커별 섹터/산업 정보 수집 (개선 버전)\n",
    "def fetch_ticker_metadata(ticker_list, batch_size=10, max_retries=5):\n",
    "    \"\"\"\n",
    "    개선된 메타데이터 수집 함수\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    failed_tickers = []\n",
    "    \n",
    "    # 캐시 파일 확인\n",
    "    cache_file = DATA_DIR / 'metadata_cache.json'\n",
    "    if cache_file.exists():\n",
    "        try:\n",
    "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                metadata = json.load(f)\n",
    "            logger.info(f\"캐시에서 {len(metadata)}개 메타데이터 로드\")\n",
    "            print(f\"  캐시에서 {len(metadata)}개 메타데이터 로드됨\")\n",
    "            \n",
    "            # 이미 수집된 티커 제외\n",
    "            ticker_list = [t for t in ticker_list if t not in metadata]\n",
    "            print(f\"→ 새로 수집할 티커: {len(ticker_list)}개\")\n",
    "            logger.info(f\"새로 수집할 티커: {len(ticker_list)}개\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"캐시 로드 실패: {str(e)}\")\n",
    "    \n",
    "    if len(ticker_list) == 0:\n",
    "        logger.info(\"모든 메타데이터가 캐시에 존재함\")\n",
    "        return metadata, failed_tickers\n",
    "    \n",
    "    chunks = [ticker_list[i:i + batch_size] for i in range(0, len(ticker_list), batch_size)]\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(tqdm(chunks, desc=\"📋 메타데이터 수집 중\")):\n",
    "        for symbol in chunk:\n",
    "            if symbol in metadata:\n",
    "                continue\n",
    "            \n",
    "            # 재시도 로직\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    ticker = yf.Ticker(symbol)\n",
    "                    info = ticker.info\n",
    "                    \n",
    "                    metadata[symbol] = {\n",
    "                        'Sector': info.get('sector', 'Unknown'),\n",
    "                        'Industry': info.get('industry', 'Unknown')\n",
    "                    }\n",
    "                    break\n",
    "                \n",
    "                except Exception as e:\n",
    "                    if attempt == max_retries - 1:\n",
    "                        logger.error(f\"메타데이터 수집 실패 - {symbol}: {str(e)}\")\n",
    "                        failed_tickers.append(symbol)\n",
    "                        metadata[symbol] = {'Sector': 'Error', 'Industry': 'Error'}\n",
    "                    else:\n",
    "                        wait_time = 2 ** attempt\n",
    "                        time.sleep(wait_time)\n",
    "            \n",
    "            time.sleep(1)  # 티커 간 대기\n",
    "        \n",
    "        # 5개 배치마다 캐시 저장\n",
    "        if chunk_idx % 5 == 0 or chunk_idx == len(chunks) - 1:\n",
    "            try:\n",
    "                with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "                logger.info(f\"캐시 저장: {len(metadata)}개\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"캐시 저장 실패: {str(e)}\")\n",
    "        \n",
    "        time.sleep(2)  # 배치 간 대기\n",
    "    \n",
    "    # 최종 캐시 저장\n",
    "    try:\n",
    "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "        logger.info(f\"최종 캐시 저장: {len(metadata)}개\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"최종 캐시 저장 실패: {str(e)}\")\n",
    "    \n",
    "    return metadata, failed_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa02a63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📋 메타데이터 수집 중:   0%|          | 0/49 [00:00<?, ?it/s]2026-01-13 13:13:54,311 - INFO - 캐시 저장: 10개\n",
      "📋 메타데이터 수집 중:  10%|█         | 5/49 [01:23<12:12, 16.65s/it]2026-01-13 13:15:17,254 - INFO - 캐시 저장: 60개\n",
      "📋 메타데이터 수집 중:  20%|██        | 10/49 [02:45<10:45, 16.55s/it]2026-01-13 13:16:39,636 - INFO - 캐시 저장: 110개\n",
      "📋 메타데이터 수집 중:  31%|███       | 15/49 [04:08<09:21, 16.52s/it]2026-01-13 13:18:01,918 - INFO - 캐시 저장: 160개\n",
      "📋 메타데이터 수집 중:  41%|████      | 20/49 [05:30<07:56, 16.42s/it]2026-01-13 13:19:24,161 - INFO - 캐시 저장: 210개\n",
      "📋 메타데이터 수집 중:  51%|█████     | 25/49 [06:52<06:37, 16.55s/it]2026-01-13 13:20:46,844 - INFO - 캐시 저장: 260개\n",
      "📋 메타데이터 수집 중:  61%|██████    | 30/49 [08:15<05:13, 16.48s/it]2026-01-13 13:22:09,070 - INFO - 캐시 저장: 310개\n",
      "📋 메타데이터 수집 중:  71%|███████▏  | 35/49 [09:37<03:50, 16.49s/it]2026-01-13 13:23:31,791 - INFO - 캐시 저장: 360개\n",
      "📋 메타데이터 수집 중:  82%|████████▏ | 40/49 [11:00<02:28, 16.49s/it]2026-01-13 13:24:54,409 - INFO - 캐시 저장: 410개\n",
      "📋 메타데이터 수집 중:  92%|█████████▏| 45/49 [12:22<01:06, 16.53s/it]2026-01-13 13:26:16,694 - INFO - 캐시 저장: 460개\n",
      "📋 메타데이터 수집 중:  98%|█████████▊| 48/49 [13:11<00:16, 16.45s/it]2026-01-13 13:26:52,985 - INFO - 캐시 저장: 481개\n",
      "📋 메타데이터 수집 중: 100%|██████████| 49/49 [13:15<00:00, 16.23s/it]\n",
      "2026-01-13 13:26:54,997 - INFO - 최종 캐시 저장: 481개\n",
      "2026-01-13 13:26:55,008 - INFO - 메타데이터 수집 완료 - 성공: 481개, 실패: 0개\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "메타데이터 수집 결과\n",
      "============================================================\n",
      "  성공: 481개\n",
      "    실패: 0개\n",
      "\n",
      "섹터 분포 (상위 10개):\n",
      "Sector\n",
      "Financial Services        82\n",
      "Technology                77\n",
      "Healthcare                56\n",
      "Industrials               56\n",
      "Consumer Cyclical         45\n",
      "Energy                    38\n",
      "Consumer Defensive        31\n",
      "Communication Services    27\n",
      "Basic Materials           26\n",
      "Utilities                 24\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 메타데이터 수집 실행\n",
    "metadata, failed_meta = fetch_ticker_metadata(tickers, batch_size=10, max_retries=5)\n",
    "metadata_df = pd.DataFrame.from_dict(metadata, orient='index').reset_index()\n",
    "metadata_df.columns = ['Company', 'Sector', 'Industry']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"메타데이터 수집 결과\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  성공: {len(metadata) - len(failed_meta)}개\")\n",
    "print(f\"    실패: {len(failed_meta)}개\")\n",
    "print(f\"\\n섹터 분포 (상위 10개):\")\n",
    "print(metadata_df['Sector'].value_counts().head(10))\n",
    "\n",
    "logger.info(f\"메타데이터 수집 완료 - 성공: {len(metadata) - len(failed_meta)}개, 실패: {len(failed_meta)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58afc7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 13:26:55,137 - INFO - 메타데이터 매핑 완료 - 최종 Shape: (860882, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "메타데이터 매핑\n",
      "============================================================\n",
      "df_raw Shape: (860882, 7)\n",
      "metadata_df Shape: (481, 3)\n",
      "\n",
      "  매핑 완료\n",
      "최종 Shape: (860882, 9)\n",
      "최종 컬럼: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Company', 'Sector', 'Industry']\n",
      "\n",
      "샘플 데이터:\n",
      "  Company       Date      Close      Sector                Industry\n",
      "0       A 2018-11-29  67.863747  Healthcare  Diagnostics & Research\n",
      "1       A 2018-11-30  68.603371  Healthcare  Diagnostics & Research\n",
      "2       A 2018-12-03  70.803230  Healthcare  Diagnostics & Research\n",
      "3       A 2018-12-04  69.134399  Healthcare  Diagnostics & Research\n",
      "4       A 2018-12-06  68.186165  Healthcare  Diagnostics & Research\n",
      "5       A 2018-12-07  66.612114  Healthcare  Diagnostics & Research\n",
      "6       A 2018-12-10  66.934502  Healthcare  Diagnostics & Research\n",
      "7       A 2018-12-11  67.370697  Healthcare  Diagnostics & Research\n",
      "8       A 2018-12-12  68.034424  Healthcare  Diagnostics & Research\n",
      "9       A 2018-12-13  67.456024  Healthcare  Diagnostics & Research\n"
     ]
    }
   ],
   "source": [
    "# 원본 데이터에 메타데이터 매핑\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"메타데이터 매핑\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 전제 조건 확인\n",
    "if 'df_raw' not in locals() or df_raw.empty:\n",
    "    logger.error(\"df_raw가 비어있습니다\")\n",
    "    raise SystemExit(\"  df_raw가 비어있습니다. Section 2를 먼저 성공적으로 실행하세요.\")\n",
    "\n",
    "if 'Company' not in df_raw.columns:\n",
    "    logger.error(\"df_raw에 'Company' 컬럼이 없습니다\")\n",
    "    raise SystemExit(f\"  df_raw에 'Company' 컬럼이 없습니다. 현재 컬럼: {df_raw.columns.tolist()}\")\n",
    "\n",
    "if metadata_df.empty:\n",
    "    logger.error(\"metadata_df가 비어있습니다\")\n",
    "    raise SystemExit(\"  metadata_df가 비어있습니다. 메타데이터를 먼저 수집하세요.\")\n",
    "\n",
    "# 매핑 전 정보\n",
    "print(f\"df_raw Shape: {df_raw.shape}\")\n",
    "print(f\"metadata_df Shape: {metadata_df.shape}\")\n",
    "\n",
    "# 매핑 실행\n",
    "df_raw = df_raw.merge(metadata_df, on='Company', how='left')\n",
    "\n",
    "# 매핑 후 정보\n",
    "print(f\"\\n  매핑 완료\")\n",
    "print(f\"최종 Shape: {df_raw.shape}\")\n",
    "print(f\"최종 컬럼: {df_raw.columns.tolist()}\")\n",
    "\n",
    "# 샘플 데이터 출력\n",
    "if all(col in df_raw.columns for col in ['Company', 'Date', 'Close', 'Sector', 'Industry']):\n",
    "    print(f\"\\n샘플 데이터:\")\n",
    "    print(df_raw[['Company', 'Date', 'Close', 'Sector', 'Industry']].head(10))\n",
    "\n",
    "logger.info(f\"메타데이터 매핑 완료 - 최종 Shape: {df_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6296fb",
   "metadata": {},
   "source": [
    "## Section 4. 데이터 품질 검증 (QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "section4_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 13:26:55,188 - INFO - Section 4 시작 - 데이터 품질 검증\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Section 4: 데이터 품질 검증\n",
      "============================================================\n",
      "  데이터 검증 통과\n",
      "  Shape: (860882, 9)\n",
      "  기업 수: 480\n",
      "  기간: 2018-11-29 00:00:00 ~ 2026-01-12 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Section 4 전제 조건 체크\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Section 4: 데이터 품질 검증\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'df_raw' not in locals() or df_raw.empty:\n",
    "    logger.error(\"df_raw가 비어있습니다\")\n",
    "    print(\"  수집된 데이터가 없습니다. Section 2를 먼저 성공적으로 실행하세요.\")\n",
    "    print(\"   또는 기존 저장된 파일을 로드하세요:\")\n",
    "    print(f\"   df_raw = pd.read_csv('{DATA_DIR / 'raw_data_backup_*.csv'}')\")\n",
    "    raise SystemExit(\"데이터 필요\")\n",
    "\n",
    "# 필수 컬럼 체크\n",
    "required_cols = ['Date', 'Company', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "missing_cols = [col for col in required_cols if col not in df_raw.columns]\n",
    "if missing_cols:\n",
    "    logger.error(f\"필수 컬럼 누락: {missing_cols}\")\n",
    "    raise SystemExit(f\"  필수 컬럼 누락: {missing_cols}\")\n",
    "\n",
    "print(\"  데이터 검증 통과\")\n",
    "print(f\"  Shape: {df_raw.shape}\")\n",
    "print(f\"  기업 수: {df_raw['Company'].nunique()}\")\n",
    "print(f\"  기간: {df_raw['Date'].min()} ~ {df_raw['Date'].max()}\")\n",
    "logger.info(\"Section 4 시작 - 데이터 품질 검증\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d1ef7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 13:26:55,230 - INFO - 관측일 부족 기업: 471개\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "기업별 관측일 수 통계\n",
      "============================================================\n",
      "         Obs_Count    Days_Span\n",
      "count   480.000000   480.000000\n",
      "mean   1793.504167  2556.318750\n",
      "std     286.821593   208.694132\n",
      "min     584.000000   851.000000\n",
      "25%    1788.000000  2601.000000\n",
      "50%    1788.000000  2601.000000\n",
      "75%    1788.000000  2601.000000\n",
      "max    3576.000000  2601.000000\n",
      "\n",
      "   관측일 부족 기업: 471개 (80% 미만)\n",
      "\n",
      "상위 10개:\n",
      "    Company  Obs_Count Start_Date\n",
      "37      ARM        584 2023-09-14\n",
      "252    KVUE        675 2023-05-04\n",
      "187    GEHC        770 2022-12-15\n",
      "269    MBLY        805 2022-10-26\n",
      "208     HLN        871 2022-07-25\n",
      "86      CEG        999 2022-01-19\n",
      "314      NU       1026 2021-12-09\n",
      "188     GFS       1055 2021-10-28\n",
      "101    COIN       1193 2021-04-14\n",
      "106    CPNG       1216 2021-03-11\n"
     ]
    }
   ],
   "source": [
    "# 기업별 관측일 수 통계\n",
    "company_stats = df_raw.groupby('Company').agg({\n",
    "    'Date': ['count', 'min', 'max']\n",
    "}).reset_index()\n",
    "company_stats.columns = ['Company', 'Obs_Count', 'Start_Date', 'End_Date']\n",
    "company_stats['Days_Span'] = (company_stats['End_Date'] - company_stats['Start_Date']).dt.days\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"기업별 관측일 수 통계\")\n",
    "print(\"=\" * 60)\n",
    "print(company_stats[['Obs_Count', 'Days_Span']].describe())\n",
    "\n",
    "# 부족한 데이터 기업\n",
    "threshold = company_stats['Obs_Count'].max() * 0.8\n",
    "partial_data = company_stats[company_stats['Obs_Count'] < threshold]\n",
    "print(f\"\\n   관측일 부족 기업: {len(partial_data)}개 (80% 미만)\")\n",
    "if len(partial_data) > 0:\n",
    "    print(\"\\n상위 10개:\")\n",
    "    print(partial_data[['Company', 'Obs_Count', 'Start_Date']].sort_values('Obs_Count').head(10))\n",
    "\n",
    "logger.info(f\"관측일 부족 기업: {len(partial_data)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "57454793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 13:26:55,252 - WARNING - OHLC 이상치: 2건\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OHLC 이상치 검증\n",
      "============================================================\n",
      "   총 이상치: 2건\n",
      "\n",
      "유형별 집계:\n",
      "Type\n",
      "Open/Close out of H/L range    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "  이상치 저장: ohlc_anomalies.csv\n"
     ]
    }
   ],
   "source": [
    "# OHLC 데이터 이상치 검증\n",
    "def validate_ohlc(df, tolerance=0.01):\n",
    "    \"\"\"OHLC 데이터 이상치 검증\"\"\"\n",
    "    anomalies = []\n",
    "    \n",
    "    # 1. 0값 또는 음수\n",
    "    for col in ['Open', 'High', 'Low', 'Close']:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        zero_or_neg = df[df[col] <= 0]\n",
    "        for _, row in zero_or_neg.iterrows():\n",
    "            anomalies.append({\n",
    "                'Company': row['Company'],\n",
    "                'Date': row['Date'],\n",
    "                'Type': f'{col} <= 0',\n",
    "                'Value': row[col]\n",
    "            })\n",
    "    \n",
    "    # 2. High < Low 역전\n",
    "    if 'High' in df.columns and 'Low' in df.columns:\n",
    "        inverted = df[df['High'] < (df['Low'] - tolerance)]\n",
    "        for _, row in inverted.iterrows():\n",
    "            anomalies.append({\n",
    "                'Company': row['Company'],\n",
    "                'Date': row['Date'],\n",
    "                'Type': 'High < Low',\n",
    "                'Value': f\"H:{row['High']:.2f}, L:{row['Low']:.2f}\"\n",
    "            })\n",
    "    \n",
    "    # 3. Open/Close가 High/Low 범위 밖\n",
    "    if all(col in df.columns for col in ['Open', 'High', 'Low', 'Close']):\n",
    "        out_of_range = df[\n",
    "            (df['Open'] > df['High'] + tolerance) | \n",
    "            (df['Open'] < df['Low'] - tolerance) |\n",
    "            (df['Close'] > df['High'] + tolerance) | \n",
    "            (df['Close'] < df['Low'] - tolerance)\n",
    "        ]\n",
    "        for _, row in out_of_range.iterrows():\n",
    "            anomalies.append({\n",
    "                'Company': row['Company'],\n",
    "                'Date': row['Date'],\n",
    "                'Type': 'Open/Close out of H/L range',\n",
    "                'Value': f\"O:{row['Open']:.2f}, H:{row['High']:.2f}, L:{row['Low']:.2f}, C:{row['Close']:.2f}\"\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(anomalies)\n",
    "\n",
    "anomaly_df = validate_ohlc(df_raw)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OHLC 이상치 검증\")\n",
    "print(\"=\" * 60)\n",
    "if len(anomaly_df) == 0:\n",
    "    print(\"  이상치 없음\")\n",
    "    logger.info(\"OHLC 이상치 없음\")\n",
    "else:\n",
    "    print(f\"   총 이상치: {len(anomaly_df)}건\")\n",
    "    print(f\"\\n유형별 집계:\")\n",
    "    print(anomaly_df['Type'].value_counts())\n",
    "    logger.warning(f\"OHLC 이상치: {len(anomaly_df)}건\")\n",
    "    \n",
    "    # 이상치 저장\n",
    "    anomaly_file = QA_DIR / 'ohlc_anomalies.csv'\n",
    "    anomaly_df.to_csv(anomaly_file, index=False)\n",
    "    print(f\"\\n  이상치 저장: {anomaly_file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd64df79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "배당 및 주식 분할 이벤트\n",
      "============================================================\n",
      "\n",
      "배당 데이터 없음\n",
      "\n",
      "주식 분할 데이터 없음\n"
     ]
    }
   ],
   "source": [
    "# Dividends & Stock Splits 이벤트 확인\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"배당 및 주식 분할 이벤트\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 배당 이벤트\n",
    "if 'Dividends' in df_raw.columns:\n",
    "    dividend_events = df_raw[df_raw['Dividends'] > 0]\n",
    "    print(f\"\\n배당 이벤트:\")\n",
    "    print(f\"  배당 기업 수: {dividend_events['Company'].nunique()}개\")\n",
    "    print(f\"  총 배당 이벤트: {len(dividend_events)}건\")\n",
    "    logger.info(f\"배당 이벤트: {len(dividend_events)}건\")\n",
    "else:\n",
    "    print(\"\\n배당 데이터 없음\")\n",
    "\n",
    "# 주식 분할 이벤트\n",
    "if 'Stock Splits' in df_raw.columns:\n",
    "    split_events = df_raw[df_raw['Stock Splits'] > 0]\n",
    "    print(f\"\\n주식 분할 이벤트:\")\n",
    "    print(f\"  분할 기업 수: {split_events['Company'].nunique()}개\")\n",
    "    print(f\"  총 분할 이벤트: {len(split_events)}건\")\n",
    "    logger.info(f\"주식 분할 이벤트: {len(split_events)}건\")\n",
    "    \n",
    "    if len(split_events) > 0:\n",
    "        print(f\"\\n상세 (상위 10건):\")\n",
    "        print(split_events[['Company', 'Date', 'Stock Splits']].head(10))\n",
    "else:\n",
    "    print(\"\\n주식 분할 데이터 없음\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da6ad7",
   "metadata": {},
   "source": [
    "## Section 5. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "section5_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 13:26:55,262 - INFO - Section 5 시작 - 데이터 정제\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Section 5: 데이터 정제\n",
      "============================================================\n",
      "  데이터 확인 완료\n"
     ]
    }
   ],
   "source": [
    "# Section 5 전제 조건 체크\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Section 5: 데이터 정제\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'df_raw' not in locals() or df_raw.empty:\n",
    "    logger.error(\"df_raw가 비어있습니다\")\n",
    "    raise SystemExit(\"  데이터가 없습니다. Section 2-4를 먼저 실행하세요.\")\n",
    "\n",
    "print(\"  데이터 확인 완료\")\n",
    "logger.info(\"Section 5 시작 - 데이터 정제\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "19c8e8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 13:26:55,331 - INFO - OHLC 정제: 2건 수정\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OHLC 범위 오류 수정\n",
      "============================================================\n",
      "수정 건수: 2\n",
      "\n",
      "수정 유형별:\n",
      "Type\n",
      "Open < Low    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "  로그 저장: ohlc_cleaning_log.csv\n"
     ]
    }
   ],
   "source": [
    "# OHLC 범위 오류 수정\n",
    "def clean_ohlc_violations(df, tolerance=0.01):\n",
    "    \"\"\"OHLC 범위 오류 수정\"\"\"\n",
    "    df_cleaned = df.copy()\n",
    "    cleaning_log = []\n",
    "    \n",
    "    # 필수 컬럼 확인\n",
    "    if not all(col in df.columns for col in ['Open', 'High', 'Low', 'Close']):\n",
    "        logger.warning(\"OHLC 컬럼 일부 누락, 정제 스킵\")\n",
    "        return df_cleaned, pd.DataFrame()\n",
    "    \n",
    "    # Open > High\n",
    "    mask = df_cleaned['Open'] > (df_cleaned['High'] + tolerance)\n",
    "    if mask.sum() > 0:\n",
    "        for idx in df_cleaned[mask].index:\n",
    "            row = df_cleaned.loc[idx]\n",
    "            cleaning_log.append({\n",
    "                'Index': idx, 'Company': row['Company'], 'Date': row['Date'],\n",
    "                'Type': 'Open > High', 'Original_High': row['High'], 'Corrected_High': row['Open']\n",
    "            })\n",
    "        df_cleaned.loc[mask, 'High'] = df_cleaned.loc[mask, 'Open']\n",
    "    \n",
    "    # Open < Low\n",
    "    mask = df_cleaned['Open'] < (df_cleaned['Low'] - tolerance)\n",
    "    if mask.sum() > 0:\n",
    "        for idx in df_cleaned[mask].index:\n",
    "            row = df_cleaned.loc[idx]\n",
    "            cleaning_log.append({\n",
    "                'Index': idx, 'Company': row['Company'], 'Date': row['Date'],\n",
    "                'Type': 'Open < Low', 'Original_Low': row['Low'], 'Corrected_Low': row['Open']\n",
    "            })\n",
    "        df_cleaned.loc[mask, 'Low'] = df_cleaned.loc[mask, 'Open']\n",
    "    \n",
    "    # Close > High\n",
    "    mask = df_cleaned['Close'] > (df_cleaned['High'] + tolerance)\n",
    "    if mask.sum() > 0:\n",
    "        for idx in df_cleaned[mask].index:\n",
    "            row = df_cleaned.loc[idx]\n",
    "            cleaning_log.append({\n",
    "                'Index': idx, 'Company': row['Company'], 'Date': row['Date'],\n",
    "                'Type': 'Close > High', 'Original_High': row['High'], 'Corrected_High': row['Close']\n",
    "            })\n",
    "        df_cleaned.loc[mask, 'High'] = df_cleaned.loc[mask, 'Close']\n",
    "    \n",
    "    # Close < Low\n",
    "    mask = df_cleaned['Close'] < (df_cleaned['Low'] - tolerance)\n",
    "    if mask.sum() > 0:\n",
    "        for idx in df_cleaned[mask].index:\n",
    "            row = df_cleaned.loc[idx]\n",
    "            cleaning_log.append({\n",
    "                'Index': idx, 'Company': row['Company'], 'Date': row['Date'],\n",
    "                'Type': 'Close < Low', 'Original_Low': row['Low'], 'Corrected_Low': row['Close']\n",
    "            })\n",
    "        df_cleaned.loc[mask, 'Low'] = df_cleaned.loc[mask, 'Close']\n",
    "    \n",
    "    return df_cleaned, pd.DataFrame(cleaning_log)\n",
    "\n",
    "df_clean, cleaning_log = clean_ohlc_violations(df_raw, tolerance=0.01)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OHLC 범위 오류 수정\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"수정 건수: {len(cleaning_log)}\")\n",
    "\n",
    "if len(cleaning_log) > 0:\n",
    "    print(f\"\\n수정 유형별:\")\n",
    "    print(cleaning_log['Type'].value_counts())\n",
    "    \n",
    "    cleaning_file = QA_DIR / 'ohlc_cleaning_log.csv'\n",
    "    cleaning_log.to_csv(cleaning_file, index=False)\n",
    "    print(f\"\\n  로그 저장: {cleaning_file.name}\")\n",
    "    logger.info(f\"OHLC 정제: {len(cleaning_log)}건 수정\")\n",
    "else:\n",
    "    print(\"  수정할 항목 없음\")\n",
    "    logger.info(\"OHLC 정제: 수정 항목 없음\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "37421b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 13:26:55,440 - INFO - 극단 변화 플래그: 17건\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  극단 변화 플래그 추가: 17건 (삭제하지 않고 보존)\n"
     ]
    }
   ],
   "source": [
    "# 극단 변화 플래그 추가\n",
    "def add_extreme_change_flag(df, threshold=0.5):\n",
    "    \"\"\"극단적 일간 변동 플래그 추가\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if 'Close' not in df.columns or 'Company' not in df.columns:\n",
    "        logger.warning(\"필수 컬럼 없음, 극단 변화 플래그 스킵\")\n",
    "        return df\n",
    "    \n",
    "    df['Daily_Change_Temp'] = df.groupby('Company')['Close'].pct_change().abs()\n",
    "    df['Is_Extreme_Change'] = df['Daily_Change_Temp'] > threshold\n",
    "    df = df.drop(columns=['Daily_Change_Temp'])\n",
    "    return df\n",
    "\n",
    "df_clean = add_extreme_change_flag(df_clean, threshold=0.5)\n",
    "extreme_count = df_clean['Is_Extreme_Change'].sum() if 'Is_Extreme_Change' in df_clean.columns else 0\n",
    "\n",
    "print(f\"\\n  극단 변화 플래그 추가: {extreme_count}건 (삭제하지 않고 보존)\")\n",
    "logger.info(f\"극단 변화 플래그: {extreme_count}건\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "abcb1867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "정제 후 재검증\n",
      "============================================================\n",
      "  OHLC 이상치 없음"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 13:26:55,458 - INFO - 정제 후 OHLC 이상치 없음\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 재검증\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"정제 후 재검증\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "anomaly_after = validate_ohlc(df_clean, tolerance=0.01)\n",
    "\n",
    "if len(anomaly_after) == 0:\n",
    "    print(\"  OHLC 이상치 없음\")\n",
    "    logger.info(\"정제 후 OHLC 이상치 없음\")\n",
    "else:\n",
    "    print(f\"   남은 이상치: {len(anomaly_after)}건\")\n",
    "    print(anomaly_after['Type'].value_counts())\n",
    "    logger.warning(f\"정제 후 남은 이상치: {len(anomaly_after)}건\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_adjusted",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정제된 데이터 저장 (adjusted 파일)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"정제된 데이터 저장\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"Shape: {df_clean.shape}\")\n",
    "if 'Date' in df_clean.columns:\n",
    "    print(f\"기간: {df_clean.min()['Date']} ~ {df_clean.max()['Date']}\")\n",
    "if 'Company' in df_clean.columns:\n",
    "    print(f\"기업 수: {df_clean['Company'].nunique()}\")\n",
    "\n",
    "try:\n",
    "    output_path = DATA_DIR / 'stock_daily_master_adjusted.csv'\n",
    "    df_clean.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\n✓ 저장 완료: {output_path.name}\")\n",
    "    print(f\"  파일 크기: {output_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    logger.info(f\"정제된 데이터 저장 완료: {output_path}\")\n",
    "    logger.info(f\"Shape: {df_clean.shape}\")\n",
    "    \n",
    "    print(\"\\n샘플 데이터 (처음 5행):\")\n",
    "    display_cols = ['Company', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    display_cols = [col for col in display_cols if col in df_clean.columns]\n",
    "    print(df_clean[display_cols].head())\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"데이터 저장 실패: {str(e)}\")\n",
    "    print(f\" 저장 중 에러: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" 데이터 수집 및 정제 완료\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n다음 단계: 04_Derived_Variable.ipynb를 실행하여 파생변수를 계산하세요.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
