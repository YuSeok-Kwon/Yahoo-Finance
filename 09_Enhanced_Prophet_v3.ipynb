{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Enhanced Prophet v3: Relative Momentum & Rank Signal\n",
    "\n",
    "---\n",
    "\n",
    "##  v3 í•µì‹¬ ê°œì„  3ê°€ì§€\n",
    "\n",
    "###  1ìˆœìœ„: ì„¹í„° ìƒëŒ€ ëª¨ë©˜í…€ (ê°€ì¥ ê°•ë ¥!)\n",
    "\n",
    "**ë¬¸ì œ**: 2023ë…„ì²˜ëŸ¼ ì „ì²´ ì‹œì¥ì´ ìƒìŠ¹í•  ë•Œ ì ˆëŒ€ ìˆ˜ìµë¥ ë¡œëŠ” êµ¬ë¶„ ë¶ˆê°€\n",
    "\n",
    "**í•´ê²°**: ì‹œì¥ ëŒ€ë¹„ ìƒëŒ€ ìˆ˜ìµë¥  ì‚¬ìš©\n",
    "```python\n",
    "RelMom_1M = Sector_Return_1M - Market_Return_1M\n",
    "RelMom_3M = Sector_Return_3M - Market_Return_3M\n",
    "RelMom_6M = Sector_Return_6M - Market_Return_6M\n",
    "RelMom_Accel = RelMom_1M - RelMom_6M\n",
    "```\n",
    "\n",
    "**íš¨ê³¼**: 2023ë…„ Techì˜ \"ì‹œì¥ ëŒ€ë¹„ ì••ë„ì  ê°•ì„¸\" í¬ì°©!\n",
    "\n",
    "###  2ìˆœìœ„: XGBoost Rank Signal ëª©í‘œ\n",
    "\n",
    "**ë¬¸ì œ**: ê¸°ì¡´ `target = residual` â†’ ì ˆëŒ€ê°’ ì˜¤ì°¨ë§Œ ì¤„ì„ â†’ ìˆœìœ„ ì˜ˆì¸¡ ì‹¤íŒ¨\n",
    "\n",
    "**í•´ê²°**: ìƒëŒ€ ê°•ë„ë¥¼ ëª©í‘œë¡œ ì‚¬ìš©\n",
    "```python\n",
    "# ì‹œì¥ í‰ê·  ëŒ€ë¹„ ìƒëŒ€ ê°•ë„\n",
    "relative_strength = Sector_Return - Market_Return\n",
    "\n",
    "# Prophetì˜ ìƒëŒ€ ì˜ˆì¸¡\n",
    "relative_yhat = Prophet_Sector - Prophet_Market\n",
    "\n",
    "# Target = ìƒëŒ€ ì”ì°¨\n",
    "target = relative_strength - relative_yhat\n",
    "```\n",
    "\n",
    "**íš¨ê³¼**: ì ˆëŒ€ ìˆ˜ìµë¥ ì€ í‹€ë ¤ë„ **ìˆœìœ„ëŠ” ë§ì¶”ëŠ” ëª¨ë¸**!\n",
    "\n",
    "###  3ìˆœìœ„: í…Œë§ˆ ê°ì§€ ë”ë¯¸\n",
    "\n",
    "**ë¬¸ì œ**: 2023 AI ë¶ì²˜ëŸ¼ íŠ¹ì • í…Œë§ˆë¡œ ì ë¦¼ ë°œìƒ ì‹œ ê°ì§€ ë¶ˆê°€\n",
    "\n",
    "**í•´ê²°**: ì§‘ì¤‘ë„ ì§€í‘œ ìƒì„±\n",
    "```python\n",
    "BigTech_Concentration = Tech ì„¹í„° ë‚´ ìƒìœ„ 3ì¢…ëª© ê±°ë˜ëŒ€ê¸ˆ ë¹„ì¤‘\n",
    "Hot_Stock_Ratio = Vol_Z_Score ìƒìœ„ 10% ì¢…ëª© ë¹„ì¤‘\n",
    "Relative_Theme_Strength = ì„¹í„° vs ì‹œì¥ í…Œë§ˆ ê°•ë„\n",
    "```\n",
    "\n",
    "**íš¨ê³¼**: í…Œë§ˆì¥ ê°ì§€ ë° ëŒ€ì‘!\n",
    "\n",
    "---\n",
    "\n",
    "##  ê¸°ëŒ€ íš¨ê³¼\n",
    "\n",
    "| ì§€í‘œ | v2 (2023) | v3 ëª©í‘œ | ê°œì„ í­ |\n",
    "|------|-----------|---------|--------|\n",
    "| **Spearman** | -0.5 ~ 0.0 | **+0.5 ~ +0.7** | **+1.0 ì´ìƒ** |\n",
    "| **Top-3 Hit** | 33% | **100%** | +67%p |\n",
    "| **Top-5 Hit** | 60% | **80%+** | +20%p |\n",
    "\n",
    "---\n",
    "\n",
    "## ì•„í‚¤í…ì²˜\n",
    "\n",
    "```\n",
    "Data Loader -> Feature Engineering (v2 + v3)\n",
    "  â†“\n",
    "v3 í”¼ì²˜:\n",
    "   Relative Momentum (RelMom_1M/3M/6M)\n",
    "   Theme Detection (BigTech_Concentration)\n",
    "  â†“\n",
    "[Rolling Loop: 2021..2025]\n",
    "  â†“\n",
    "Prophet (v3 regressors í¬í•¨)\n",
    "  â†“\n",
    "XGBoost ( Rank Signal ëª©í‘œ)\n",
    "  â†“\n",
    "Hybrid Prediction (Prophet + XGB Correction)\n",
    "  â†“\n",
    "Evaluation (Spearman, Top-K Hit Rate)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. í™˜ê²½ ì„¤ì •\n",
    "\n",
    "### ì˜ë„/ëª©ì \n",
    "- í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ\n",
    "- **v3 ëª¨ë“ˆ** import: `v3_features.py`ì˜ í•µì‹¬ í•¨ìˆ˜ë“¤\n",
    "- í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "\n",
    "### ì½”ë“œ ì„¤ëª…\n",
    "- `v3_features`: ìƒëŒ€ ëª¨ë©˜í…€, í…Œë§ˆ ê°ì§€ í”¼ì²˜ ìƒì„±\n",
    "- `enhanced_features`: v2 í”¼ì²˜ (ê±°ì‹œê²½ì œ, ë ˆì§, ëª¨ë©˜í…€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Font: NanumGothic\n",
      "\n",
      " v3 ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "from prophet import Prophet\n",
    "import xgboost as xgb\n",
    "import holidays\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sector_rotation_features import (\n",
    "    load_and_prepare_macro,\n",
    "    merge_macro_to_sector_panel,\n",
    "    add_regime_features,\n",
    "    add_momentum_features,\n",
    "    calculate_dynamic_cps,\n",
    "    add_relative_momentum_features,      #  ìƒëŒ€ ëª¨ë©˜í…€\n",
    "    add_theme_detection_features,        #  í…Œë§ˆ ê°ì§€\n",
    "    prepare_rank_signal_target,          #  Rank Signal\n",
    "    apply_v3_enhancements                # í†µí•© í•¨ìˆ˜\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ì‹œê°í™” ìŠ¤íƒ€ì¼\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 7)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# í•œê¸€ í°íŠ¸\n",
    "import koreanize_matplotlib\n",
    "pkg_dir = os.path.dirname(koreanize_matplotlib.__file__)\n",
    "font_path = os.path.join(pkg_dir, 'fonts', 'NanumGothic.ttf')\n",
    "\n",
    "fm.fontManager.addfont(font_path)\n",
    "font_name = fm.FontProperties(fname=font_path).get_name()\n",
    "plt.rcParams['font.family'] = font_name\n",
    "plt.rcParams['font.sans-serif'] = [font_name] + plt.rcParams['font.sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"Font: {font_name}\")\n",
    "print(\"\\n v3 ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "paths",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í”„ë¡œì íŠ¸ ê²½ë¡œ: /Users/yu_seok/Documents/workspace/nbCamp/Project/Yahoo Finance\n",
      "ê²°ê³¼ ì €ì¥: /Users/yu_seok/Documents/workspace/nbCamp/Project/Yahoo Finance/Data_set/Integrated_Prophet_Results\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = Path('.').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / 'Data_set'\n",
    "OUTPUT_DIR = DATA_DIR / 'Integrated_Prophet_Results'\n",
    "TABLEAU_DIR = DATA_DIR / 'Tableau_Csv'\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"í”„ë¡œì íŠ¸ ê²½ë¡œ: {PROJECT_ROOT}\")\n",
    "print(f\"ê²°ê³¼ ì €ì¥: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-load",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2. ë°ì´í„° ë¡œë“œ ë° ì„¹í„° ì¸ë±ìŠ¤ ìƒì„±\n",
    "\n",
    "### 2.1 ì›ë³¸ ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ë°ì´í„° ë¡œë“œ\n",
      "============================================================\n",
      "\n",
      "ë¡œë“œ ì™„ë£Œ\n",
      "  Shape: (603359, 41)\n",
      "  ê¸°ê°„: 2020-11-27 ~ 2026-01-09\n",
      "  ê¸°ì—… ìˆ˜: 481\n",
      "  ì„¹í„° ìˆ˜: 11\n",
      "\n",
      "ì„¹í„° ëª©ë¡:\n",
      "   1. Financial Services (82ê°œ ê¸°ì—…)\n",
      "   2. Technology (77ê°œ ê¸°ì—…)\n",
      "   3. Healthcare (56ê°œ ê¸°ì—…)\n",
      "   4. Industrials (56ê°œ ê¸°ì—…)\n",
      "   5. Consumer Cyclical (45ê°œ ê¸°ì—…)\n",
      "   6. Energy (38ê°œ ê¸°ì—…)\n",
      "   7. Consumer Defensive (31ê°œ ê¸°ì—…)\n",
      "   8. Communication Services (27ê°œ ê¸°ì—…)\n",
      "   9. Basic Materials (26ê°œ ê¸°ì—…)\n",
      "  10. Utilities (24ê°œ ê¸°ì—…)\n",
      "  11. Real Estate (19ê°œ ê¸°ì—…)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ë°ì´í„° ë¡œë“œ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df = pd.read_csv(DATA_DIR / 'stock_features_clean.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ë° Unknown ì„¹í„° ì œê±°\n",
    "df = df.dropna(subset=['Date', 'Close', 'Sector', 'Daily_Return'])\n",
    "df = df[df['Sector'] != 'Unknown']\n",
    "\n",
    "print(f\"\\në¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  ê¸°ê°„: {df['Date'].min().date()} ~ {df['Date'].max().date()}\")\n",
    "print(f\"  ê¸°ì—… ìˆ˜: {df['Company'].nunique()}\")\n",
    "print(f\"  ì„¹í„° ìˆ˜: {df['Sector'].nunique()}\")\n",
    "\n",
    "# ì„¹í„° ëª©ë¡\n",
    "print(\"\\nì„¹í„° ëª©ë¡:\")\n",
    "sector_counts = df.groupby('Sector')['Company'].nunique().sort_values(ascending=False)\n",
    "for i, (sector, count) in enumerate(sector_counts.items(), 1):\n",
    "    print(f\"  {i:2d}. {sector} ({count}ê°œ ê¸°ì—…)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sector-index",
   "metadata": {},
   "source": [
    "### 2.2 ì„¹í„° ì¸ë±ìŠ¤ ìƒì„± (ìˆ˜ìµë¥  ê¸°ë°˜)\n",
    "\n",
    "**ê³„ì‚° ë°©ì‹**:\n",
    "1. ì„¹í„° ë‚´ ëª¨ë“  ì¢…ëª©ì˜ ì¼ê°„ ìˆ˜ìµë¥  í‰ê·  ê³„ì‚°\n",
    "2. ì´ë¥¼ 100ì—ì„œ ì‹œì‘í•˜ëŠ” ëˆ„ì  ì§€ìˆ˜ë¡œ í™˜ì‚°\n",
    "\n",
    "**ì¥ì **: \n",
    "- ê³ ê°€ ì£¼ì‹ í¸í–¥ ì œê±°\n",
    "- Prophetì´ ì„ í˜• ì¶”ì„¸ ì˜ í¬ì°©\n",
    "- XGBoost ì”ì°¨ ëª¨ë¸ë„ ì•ˆì •ì "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "calc-sector-index",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ì„¹í„° ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: (14135, 8)\n",
      "ì»¬ëŸ¼: ['Date', 'Sector', 'Daily_Return', 'Volume', 'Company', 'Index', 'y', 'ds']\n",
      "\n",
      "ìƒ˜í”Œ ë°ì´í„°:\n",
      "         Date           Sector  Daily_Return     Volume  Company       Index  \\\n",
      "0  2020-11-27  Basic Materials      0.003054   68568783       24  100.305401   \n",
      "11 2020-11-30  Basic Materials     -0.009052  132420442       24   99.397448   \n",
      "22 2020-12-01  Basic Materials      0.022079  126777499       24  101.592009   \n",
      "33 2020-12-02  Basic Materials     -0.000330  123427550       24  101.558435   \n",
      "44 2020-12-03  Basic Materials      0.003853  107596029       24  101.949698   \n",
      "55 2020-12-04  Basic Materials      0.019749  127260672       24  103.963087   \n",
      "66 2020-12-07  Basic Materials     -0.000743  105091051       24  103.885852   \n",
      "77 2020-12-08  Basic Materials      0.003298   82921991       24  104.228511   \n",
      "88 2020-12-09  Basic Materials     -0.005910   87466621       24  103.612512   \n",
      "99 2020-12-10  Basic Materials     -0.001343  101516656       24  103.473313   \n",
      "\n",
      "           y         ds  \n",
      "0   4.608220 2020-11-27  \n",
      "11  4.599126 2020-11-30  \n",
      "22  4.620965 2020-12-01  \n",
      "33  4.620634 2020-12-02  \n",
      "44  4.624480 2020-12-03  \n",
      "55  4.644036 2020-12-04  \n",
      "66  4.643293 2020-12-07  \n",
      "77  4.646586 2020-12-08  \n",
      "88  4.640658 2020-12-09  \n",
      "99  4.639314 2020-12-10  \n"
     ]
    }
   ],
   "source": [
    "def calculate_sector_index_log(df):\n",
    "    \"\"\"\n",
    "    ìˆ˜ìµë¥  ê¸°ë°˜ ì„¹í„° ì¸ë±ìŠ¤ ê³„ì‚° (Vectorized & Log Consistency)\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    #  1. ì´ìƒì¹˜ í´ë¦¬í•‘\n",
    "    # ê°œë³„ ì¢…ëª©ì˜ ìˆ˜ìµë¥ ì´ ìƒìœ„ 1% / í•˜ìœ„ 1%ë¥¼ ë²—ì–´ë‚˜ë©´ ê²½ê³„ê°’ìœ¼ë¡œ ì¹˜í™˜\n",
    "    upper_limit = df_clean['Daily_Return'].quantile(0.99)\n",
    "    lower_limit = df_clean['Daily_Return'].quantile(0.01)\n",
    "    \n",
    "    # ì˜ˆ: ìƒìœ„ 1%ê°€ +25%ë¼ë©´, +50%ì¸ ì¢…ëª©ì„ +25%ë¡œ ê¹ìŒ\n",
    "    df_clean['Daily_Return'] = df_clean['Daily_Return'].clip(lower=lower_limit, upper=upper_limit)\n",
    "\n",
    "    # 2. ì„¹í„°/ì¼ìë³„ ì§‘ê³„\n",
    "    sector_daily = df_clean.groupby(['Date', 'Sector'], as_index=False).agg({\n",
    "        'Daily_Return': 'mean',\n",
    "        'Volume': 'sum',\n",
    "        'Company': 'count'\n",
    "    })\n",
    "    \n",
    "    # ë‚ ì§œìˆœ ì •ë ¬\n",
    "    sector_daily = sector_daily.sort_values(['Sector', 'Date'])\n",
    "    \n",
    "    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "    sector_daily['Daily_Return'] = sector_daily['Daily_Return'].fillna(0)\n",
    "    \n",
    "    # 3. ì§€ìˆ˜ ê³„ì‚°\n",
    "    # ê° ì„¹í„°ë³„ë¡œ (1+r)ì˜ ëˆ„ì ê³± ê³„ì‚°\n",
    "    sector_daily['Index'] = sector_daily.groupby('Sector')['Daily_Return'].transform(\n",
    "        lambda x: 100 * (1 + x).cumprod()\n",
    "    )\n",
    "    \n",
    "    # 4. ë¡œê·¸ ë³€í™˜\n",
    "    sector_daily['y'] = np.log(sector_daily['Index'])\n",
    "    \n",
    "    # Prophet ì…ë ¥ìš© ì»¬ëŸ¼\n",
    "    sector_daily['ds'] = sector_daily['Date']\n",
    "    \n",
    "    return sector_daily\n",
    "\n",
    "# ì‹¤í–‰\n",
    "sector_df = calculate_sector_index_log(df)\n",
    "\n",
    "print(f\"\\nì„¹í„° ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {sector_df.shape}\")\n",
    "print(f\"ì»¬ëŸ¼: {sector_df.columns.tolist()}\")\n",
    "print(f\"\\nìƒ˜í”Œ ë°ì´í„°:\")\n",
    "print(sector_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "features",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3. Feature Engineering (ê¸°ìˆ ì  ì§€í‘œ)\n",
    "\n",
    "### 3.1 ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°\n",
    "\n",
    "**Prophet ì™¸ë¶€ ë³€ìˆ˜ë¡œ ì‚¬ìš©í•  ê¸°ìˆ ì  ì§€í‘œ**:\n",
    "- **RSI_14**: ìƒëŒ€ê°•ë„ì§€ìˆ˜ (ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„)\n",
    "- **Volatility_20d**: 20ì¼ ë³€ë™ì„±\n",
    "- **BB_Width**: ë³¼ë¦°ì € ë°´ë“œ í­ (ë³€ë™ì„± ìŠ¤í€´ì¦ˆ)\n",
    "- **Vol_Z_Score**: ê±°ë˜ëŸ‰ Z-Score\n",
    "- **Gap_Pct**: ì „ì¼ ëŒ€ë¹„ ê°­ ë¹„ìœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feature-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def build_sector_panel_from_stock_df(\n",
    "    df: pd.DataFrame,\n",
    "    sector_index_df: pd.DataFrame,\n",
    "    key_columns: list,\n",
    "    *,\n",
    "    # ì§‘ê³„/ê°€ê³µ ì˜µì…˜\n",
    "    mean_cols: list | None = None,\n",
    "    risk_cols_q80: list | None = None,\n",
    "    vol_z_q90_col: str = \"Vol_Z_Score\",\n",
    "    prefix: str = \"Avg_\",\n",
    "    # íŒŒìƒ ì»¬ëŸ¼ ìƒì„± ì˜µì…˜\n",
    "    add_scaled_cols: bool = True,\n",
    "    add_log1p_cols: bool = True,\n",
    "    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "    fill_method: str = \"ffill_bfill\",   # \"ffill_bfill\" | \"ffill_zero\" | \"none\"\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # 0) ì•ˆì „ì¥ì¹˜ / í‘œì¤€í™”\n",
    "    df = df.copy()\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "    sector_index_df = sector_index_df.copy()\n",
    "    if \"Date\" not in sector_index_df.columns and \"ds\" in sector_index_df.columns:\n",
    "        sector_index_df[\"Date\"] = pd.to_datetime(sector_index_df[\"ds\"])\n",
    "    else:\n",
    "        sector_index_df[\"Date\"] = pd.to_datetime(sector_index_df[\"Date\"])\n",
    "\n",
    "    if \"ds\" not in sector_index_df.columns:\n",
    "        sector_index_df[\"ds\"] = sector_index_df[\"Date\"]\n",
    "\n",
    "    # 1) ê¸°ë³¸ ì§‘ê³„ ëŒ€ìƒ ì»¬ëŸ¼ ì •ì˜\n",
    "    default_mean_cols = [\n",
    "        \"Daily_Return_raw\", \"Cum_Return\",\n",
    "        \"Return_1M\", \"Return_3M\", \"Return_6M\",\n",
    "        \"MA_5\", \"MA_20\", \"MA_60\",\n",
    "        \"Volatility_20d\",\n",
    "        \"Vol_MA_20\", \"Vol_Ratio\", \"Log_Volume_W\",\n",
    "        \"RSI_14\", \"BB_Width\",\n",
    "    ]\n",
    "    default_risk_cols_q80 = [\"Drawdown\", \"MDD\", \"DD_Short\"]\n",
    "\n",
    "    if mean_cols is None:\n",
    "        mean_cols = default_mean_cols\n",
    "    if risk_cols_q80 is None:\n",
    "        risk_cols_q80 = default_risk_cols_q80\n",
    "\n",
    "    mean_cols = [c for c in mean_cols if (c in key_columns and c in df.columns)]\n",
    "    risk_cols_q80 = [c for c in risk_cols_q80 if (c in key_columns and c in df.columns)]\n",
    "    use_vol_z_q90 = (vol_z_q90_col in key_columns and vol_z_q90_col in df.columns)\n",
    "\n",
    "    # 2) ì§‘ê³„ ì‹¤í–‰\n",
    "    # mean ì§‘ê³„\n",
    "    frames = []\n",
    "    if mean_cols:\n",
    "        mean_agg = (\n",
    "            df.groupby([\"Date\", \"Sector\"], as_index=False)[mean_cols]\n",
    "              .mean()\n",
    "        )\n",
    "        frames.append(mean_agg)\n",
    "\n",
    "    # q80(ë³´ìˆ˜ì ) ì§‘ê³„ - ë¦¬ìŠ¤í¬ ì»¬ëŸ¼\n",
    "    if risk_cols_q80:\n",
    "        q80_agg = (\n",
    "            df.groupby([\"Date\", \"Sector\"], as_index=False)[risk_cols_q80]\n",
    "              .quantile(0.80)\n",
    "        )\n",
    "        frames.append(q80_agg)\n",
    "\n",
    "    # q90 ì§‘ê³„ - Vol_Z_Score (ì„¹í„° ë‚´ 'ê¸‰ì¦ ì¢…ëª© ì¡´ì¬' ì‹ í˜¸ ì‚´ë¦¬ê¸°)\n",
    "    if use_vol_z_q90:\n",
    "        q90_agg = (\n",
    "            df.groupby([\"Date\", \"Sector\"], as_index=False)[[vol_z_q90_col]]\n",
    "              .quantile(0.90)\n",
    "        )\n",
    "        frames.append(q90_agg)\n",
    "\n",
    "    if not frames:\n",
    "        sector_panel = sector_index_df.sort_values([\"Sector\", \"ds\"]).copy()\n",
    "        return sector_panel\n",
    "\n",
    "    # ì§‘ê³„ ê²°ê³¼ ë³‘í•© (Date, Sector ê¸°ì¤€)\n",
    "    sector_feats = frames[0]\n",
    "    for f in frames[1:]:\n",
    "        sector_feats = sector_feats.merge(f, on=[\"Date\", \"Sector\"], how=\"outer\")\n",
    "\n",
    "    # 3) ì»¬ëŸ¼ëª… ì ‘ë‘ì–´(Avg_) ì ìš©\n",
    "    rename_map = {}\n",
    "    for c in mean_cols + risk_cols_q80:\n",
    "        if c in sector_feats.columns:\n",
    "            rename_map[c] = f\"{prefix}{c}\"\n",
    "\n",
    "    if use_vol_z_q90 and vol_z_q90_col in sector_feats.columns:\n",
    "        rename_map[vol_z_q90_col] = f\"{prefix}{vol_z_q90_col}\"\n",
    "\n",
    "    sector_feats = sector_feats.rename(columns=rename_map)\n",
    "\n",
    "    # 4) ì„¹í„° ì§€ìˆ˜(íƒ€ê¹ƒ y)ì™€ ê²°í•©\n",
    "    sector_panel = sector_index_df.merge(\n",
    "        sector_feats,\n",
    "        on=[\"Date\", \"Sector\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 5) ëª¨ë¸ ì…ë ¥ìš© íŒŒìƒ ì»¬ëŸ¼ (ìŠ¤ì¼€ì¼/ë¡œê·¸)\n",
    "    if add_scaled_cols and f\"{prefix}RSI_14\" in sector_panel.columns:\n",
    "        sector_panel[f\"{prefix}RSI_14_scaled\"] = sector_panel[f\"{prefix}RSI_14\"] / 100.0\n",
    "\n",
    "    if add_log1p_cols:\n",
    "        for base_col in [\"Volatility_20d\", \"Vol_Ratio\", \"Log_Volume_W\"]:\n",
    "            c = f\"{prefix}{base_col}\"\n",
    "            if c in sector_panel.columns:\n",
    "                sector_panel[f\"{c}_log1p\"] = np.log1p(sector_panel[c])\n",
    "\n",
    "    # 6) ì •ë ¬ + ê²°ì¸¡ ì²˜ë¦¬\n",
    "    sector_panel = sector_panel.sort_values([\"Sector\", \"ds\"])\n",
    "\n",
    "    if fill_method == \"ffill_bfill\":\n",
    "        sector_panel = sector_panel.groupby(\"Sector\", group_keys=False).apply(\n",
    "            lambda x: x.ffill().bfill()\n",
    "        )\n",
    "    elif fill_method == \"ffill_zero\":\n",
    "        sector_panel = sector_panel.groupby(\"Sector\", group_keys=False).apply(\n",
    "            lambda x: x.ffill()\n",
    "        ).fillna(0)\n",
    "    elif fill_method == \"none\":\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"fill_method must be one of: 'ffill_bfill', 'ffill_zero', 'none'\")\n",
    "\n",
    "    return sector_panel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ì„¹í„° íŒ¨ë„ ìƒì„± (ê¸°ìˆ ì  ì§€í‘œ ì§‘ê³„)\n",
    "\n",
    "ì¢…ëª©ë³„ ê¸°ìˆ ì  ì§€í‘œë¥¼ ì„¹í„°ë³„ë¡œ ì§‘ê³„í•˜ì—¬ sector_panel ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„¹í„° íŒ¨ë„ ìƒì„± ì™„ë£Œ: (14135, 30)\n",
      "ì»¬ëŸ¼ ê°œìˆ˜: 30\n"
     ]
    }
   ],
   "source": [
    "key_columns = [\n",
    "    'Daily_Return_raw','Cum_Return',\n",
    "    'Return_1M','Return_3M','Return_6M',\n",
    "    'MA_5','MA_20','MA_60',\n",
    "    'Volatility_20d',\n",
    "    'Vol_MA_20','Vol_Ratio','Log_Volume_W',\n",
    "    'RSI_14','BB_Width',\n",
    "    'Drawdown','MDD','DD_Short', 'Vol_Z_Score'\n",
    "]\n",
    "\n",
    "sector_panel = build_sector_panel_from_stock_df(\n",
    "    df=df,\n",
    "    sector_index_df=sector_df,\n",
    "    key_columns=key_columns,\n",
    "    prefix=\"Avg_\",\n",
    "    add_scaled_cols=True,\n",
    "    add_log1p_cols=True,\n",
    "    fill_method=\"ffill_bfill\"\n",
    ")\n",
    "\n",
    "print(f\"ì„¹í„° íŒ¨ë„ ìƒì„± ì™„ë£Œ: {sector_panel.shape}\")\n",
    "print(f\"ì»¬ëŸ¼ ê°œìˆ˜: {len(sector_panel.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4. Feature Engineering: v2 + v3 í†µí•©\n",
    "\n",
    "### ì˜ë„/ëª©ì \n",
    "- **v2 í”¼ì²˜**: ê±°ì‹œê²½ì œ(DGS10, WTI), ë ˆì§(Market_Vol), ëª¨ë©˜í…€(Mom_1M/6M)\n",
    "- ** v3 í”¼ì²˜**: ìƒëŒ€ ëª¨ë©˜í…€, í…Œë§ˆ ê°ì§€\n",
    "\n",
    "### ì½”ë“œ ì„¤ëª…\n",
    "1. v2 í”¼ì²˜ ìƒì„± (ê¸°ì¡´ ë°©ì‹)\n",
    "2.  ìƒëŒ€ ëª¨ë©˜í…€ ì¶”ê°€: `add_relative_momentum_features()`\n",
    "3.  í…Œë§ˆ ê°ì§€ ì¶”ê°€: `add_theme_detection_features()`\n",
    "4. ê²°ê³¼: ì´ ~35ê°œ í”¼ì²˜ (v2 25ê°œ + v3 10ê°œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Feature Engineering: v2 + v3\n",
      "======================================================================\n",
      "\n",
      "[1/3] v2 í”¼ì²˜ ìƒì„± ì¤‘...\n",
      "======================================================================\n",
      "ê±°ì‹œê²½ì œ ë°ì´í„° ë¡œë“œ\n",
      "======================================================================\n",
      "  íŒŒì¼: /Users/yu_seok/Documents/workspace/nbCamp/Project/Yahoo Finance/Data_set/macro_data.csv\n",
      "  ê¸°ê°„: 2018-01-01 00:00:00 ~ 2026-01-14 00:00:00\n",
      "  ë ˆì½”ë“œ: 2,936\n",
      "\n",
      "ìƒì„±ëœ íŒŒìƒ í”¼ì²˜:\n",
      "  [DGS10] DGS10_level, DGS10_change_20d, DGS10_zscore_252\n",
      "  [WTI] WTI_log, WTI_mom_3M, WTI_change_20d\n",
      "\n",
      " ê±°ì‹œê²½ì œ ë°ì´í„° ë³‘í•© ì™„ë£Œ\n",
      "  ì¶”ê°€ëœ ì»¬ëŸ¼: 6ê°œ\n",
      "  ìµœì¢… Shape: (14135, 36)\n",
      "\n",
      " ë ˆì§ í”¼ì²˜ ì¶”ê°€ ì™„ë£Œ\n",
      "  - Market_Vol, CrossSection_Dispersion\n",
      "\n",
      " ëª¨ë©˜í…€ í”¼ì²˜ ì¶”ê°€ ì™„ë£Œ\n",
      "  - Mom_1M, Mom_6M, Mom_Accel\n",
      "   v2 í”¼ì²˜ ì™„ë£Œ: (14135, 41)\n",
      "  v2 í”¼ì²˜ ê°œìˆ˜: ~32ê°œ\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Feature Engineering: v2 + v3\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# =========================================================================\n",
    "# v2 í”¼ì²˜ ìƒì„± (ê¸°ì¡´)\n",
    "# =========================================================================\n",
    "print(\"\\n[1/3] v2 í”¼ì²˜ ìƒì„± ì¤‘...\")\n",
    "\n",
    "# ê±°ì‹œê²½ì œ ë°ì´í„° ë¡œë“œ\n",
    "macro_df = load_and_prepare_macro(DATA_DIR)\n",
    "sector_panel = merge_macro_to_sector_panel(sector_panel, macro_df)\n",
    "\n",
    "# ë ˆì§ í”¼ì²˜\n",
    "sector_panel = add_regime_features(sector_panel)\n",
    "\n",
    "# ëª¨ë©˜í…€ í”¼ì²˜\n",
    "sector_panel = add_momentum_features(sector_panel)\n",
    "\n",
    "print(f\"   v2 í”¼ì²˜ ì™„ë£Œ: {sector_panel.shape}\")\n",
    "v2_cols = [c for c in sector_panel.columns if any(x in c for x in ['DGS10', 'WTI', 'Market_Vol', 'Mom_', 'Avg_'])]\n",
    "print(f\"  v2 í”¼ì²˜ ê°œìˆ˜: ~{len(v2_cols)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/3]  v3 ìƒëŒ€ ëª¨ë©˜í…€ í”¼ì²˜ ìƒì„± ì¤‘...\n",
      "\n",
      " ì„¹í„° ìƒëŒ€ ëª¨ë©˜í…€ í”¼ì²˜ ì¶”ê°€ ì™„ë£Œ\n",
      "  - Relative_Mom_21d/63d/126d_diff/ratio\n",
      "  - RelMom_1M/3M/6M, RelMom_Accel\n",
      "  ğŸ“Š 2023ë…„ Tech ê°™ì€ 'ì‹œì¥ ëŒ€ë¹„ ê°•ì„¸' í¬ì°©!\n",
      "\n",
      "[3/3]  v3 í…Œë§ˆ ê°ì§€ í”¼ì²˜ ìƒì„± ì¤‘...\n",
      "\n",
      " í…Œë§ˆ ê°ì§€ í”¼ì²˜ ì¶”ê°€ ì™„ë£Œ\n",
      "  - BigTech_Concentration, Hot_Stock_Ratio\n",
      "  - Relative_Theme_Strength\n",
      "  ğŸ“Š 2023 AI ë¶ ê°™ì€ í…Œë§ˆì¥ íƒì§€!\n",
      "\n",
      "======================================================================\n",
      " Feature Engineering ì™„ë£Œ\n",
      "======================================================================\n",
      "ìµœì¢… Shape: (14135, 63)\n",
      "\n",
      " v3 í”¼ì²˜ ê°œìˆ˜: 14ê°œ\n",
      "  ì£¼ìš” v3 í”¼ì²˜:\n",
      "    - RelMom_1M\n",
      "    - RelMom_3M\n",
      "    - RelMom_6M\n",
      "    - RelMom_Accel\n",
      "    - BigTech_Concentration\n",
      "    - Hot_Stock_Ratio\n",
      "    - Relative_Theme_Strength\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "#  v3 í”¼ì²˜ ìƒì„±\n",
    "# =========================================================================\n",
    "print(\"\\n[2/3]  v3 ìƒëŒ€ ëª¨ë©˜í…€ í”¼ì²˜ ìƒì„± ì¤‘...\")\n",
    "\n",
    "#  1ìˆœìœ„: ì„¹í„° ìƒëŒ€ ëª¨ë©˜í…€\n",
    "sector_panel = add_relative_momentum_features(\n",
    "    sector_panel,\n",
    "    windows=[21, 63, 126],  # 1M, 3M, 6M\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n[3/3]  v3 í…Œë§ˆ ê°ì§€ í”¼ì²˜ ìƒì„± ì¤‘...\")\n",
    "\n",
    "#  3ìˆœìœ„: í…Œë§ˆ ê°ì§€\n",
    "sector_panel = add_theme_detection_features(\n",
    "    df,\n",
    "    sector_panel,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" Feature Engineering ì™„ë£Œ\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"ìµœì¢… Shape: {sector_panel.shape}\")\n",
    "\n",
    "# v3 í”¼ì²˜ í™•ì¸\n",
    "v3_cols = [c for c in sector_panel.columns if any(x in c for x in ['RelMom', 'Relative_Mom', 'BigTech', 'Hot_Stock', 'Theme'])]\n",
    "print(f\"\\n v3 í”¼ì²˜ ê°œìˆ˜: {len(v3_cols)}ê°œ\")\n",
    "print(\"  ì£¼ìš” v3 í”¼ì²˜:\")\n",
    "for col in ['RelMom_1M', 'RelMom_3M', 'RelMom_6M', 'RelMom_Accel', 'BigTech_Concentration', 'Hot_Stock_Ratio', 'Relative_Theme_Strength']:\n",
    "    if col in sector_panel.columns:\n",
    "        print(f\"    - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5. Prophet ëª¨ë¸ ì •ì˜ (ì¶”ì„¸ í•™ìŠµ)\n",
    "\n",
    "### ì˜ë„/ëª©ì \n",
    "- Prophet: **ì¶”ì„¸ + ê³„ì ˆì„±ë§Œ í•™ìŠµ** (íšŒê·€ë³€ìˆ˜ ì—†ìŒ)\n",
    "- XGBoost: v3 í”¼ì²˜ë¡œ ì”ì°¨ ë³´ì •\n",
    "- ì—­í•  ë¶„ë¦¬: Prophet(ì¥ê¸° ì¶”ì„¸) + XGBoost(ë‹¨ê¸° íŒ¨í„´)\n",
    "\n",
    "### v3 í•µì‹¬\n",
    "- Prophetì€ ê¹”ë”í•œ ì¶”ì„¸ë§Œ í•™ìŠµ\n",
    "- ** ìƒëŒ€ ëª¨ë©˜í…€**, ** Rank Signal**, ** í…Œë§ˆ ê°ì§€**ëŠ” ëª¨ë‘ XGBoostì—ì„œ í™œìš©\n",
    "- ì„¹í„°ë³„ ë™ì  changepoint prior ì¡°ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " v3 Prophet ëª¨ë¸ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n",
      "   Prophet: ì¶”ì„¸ + ê³„ì ˆì„±ë§Œ í•™ìŠµ\n",
      "   XGBoost: v3 í”¼ì²˜ë¡œ ì”ì°¨ ë³´ì •\n"
     ]
    }
   ],
   "source": [
    "# ë¯¸êµ­ ê³µíœ´ì¼\n",
    "US_HOLIDAYS = pd.DataFrame({\n",
    "    'holiday': 'US_Holiday',\n",
    "    'ds': pd.to_datetime([\n",
    "        '2021-01-01', '2021-07-04', '2021-12-25',\n",
    "        '2022-01-01', '2022-07-04', '2022-12-25',\n",
    "        '2023-01-01', '2023-07-04', '2023-12-25',\n",
    "        '2024-01-01', '2024-07-04', '2024-12-25',\n",
    "        '2025-01-01', '2025-07-04', '2025-12-25',\n",
    "    ]),\n",
    "    'lower_window': 0,\n",
    "    'upper_window': 1,\n",
    "})\n",
    "\n",
    "def create_v3_prophet_model(holidays_df, sector_data=None, base_cps=0.05):\n",
    "    \"\"\"\n",
    "    v3 Prophet ëª¨ë¸ ìƒì„± (ì¶”ì„¸ë§Œ í•™ìŠµ)\n",
    "    \n",
    "    Prophet: ì¶”ì„¸ + ê³„ì ˆì„±ë§Œ í•™ìŠµ\n",
    "    XGBoost: v3 í”¼ì²˜ë¡œ ì”ì°¨ ë³´ì •\n",
    "    \"\"\"\n",
    "    \n",
    "    # ì„¹í„°ë³„ ë³€ë™ì„±ì— ë”°ë¥¸ ë™ì  cps ì¡°ì •\n",
    "    if sector_data is not None:\n",
    "        sector_vol = sector_data.get('Avg_Volatility_20d', pd.Series([0.02])).mean()\n",
    "        market_vol = sector_data.get('Market_Vol', pd.Series([0.02])).mean()\n",
    "        \n",
    "        if market_vol > 0:\n",
    "            relative_vol = sector_vol / market_vol\n",
    "            if relative_vol > 1.2:\n",
    "                cps = base_cps * 1.5\n",
    "            elif relative_vol < 0.8:\n",
    "                cps = base_cps * 0.7\n",
    "            else:\n",
    "                cps = base_cps\n",
    "        else:\n",
    "            cps = base_cps\n",
    "    else:\n",
    "        cps = base_cps\n",
    "    \n",
    "    # Prophet ëª¨ë¸ ìƒì„± (íšŒê·€ë³€ìˆ˜ ì—†ìŒ)\n",
    "    model = Prophet(\n",
    "        growth='linear',\n",
    "        changepoint_prior_scale=cps,\n",
    "        seasonality_prior_scale=10.0,\n",
    "        holidays_prior_scale=10.0,\n",
    "        seasonality_mode='additive',\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False,\n",
    "        holidays=holidays_df,\n",
    "        interval_width=0.95\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\" v3 Prophet ëª¨ë¸ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"   Prophet: ì¶”ì„¸ + ê³„ì ˆì„±ë§Œ í•™ìŠµ\")\n",
    "print(\"   XGBoost: v3 í”¼ì²˜ë¡œ ì”ì°¨ ë³´ì •\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6. XGBoost ëª¨ë¸ ( v3 Rank Signal)\n",
    "\n",
    "### ì˜ë„/ëª©ì \n",
    "-  **Rank Signal Target**: ìƒëŒ€ ê°•ë„ ìµœì í™”\n",
    "- ê¸°ì¡´: `target = y - yhat` (ì ˆëŒ€ ì”ì°¨)\n",
    "- **v3**: `target = (y - market_y) - (yhat - market_yhat)` (ìƒëŒ€ ì”ì°¨)\n",
    "\n",
    "### íš¨ê³¼\n",
    "- 2023ë…„ì²˜ëŸ¼ ì „ì²´ ì‹œì¥ì´ ìƒìŠ¹í•  ë•Œë„ ìˆœìœ„ ì˜ˆì¸¡ ê°€ëŠ¥\n",
    "- Spearman ìƒê´€ê³„ìˆ˜ ëŒ€í­ ê°œì„  (-0.5 â†’ +0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XGBoost í”¼ì²˜ ìƒì„± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def make_xgb_features(df: pd.DataFrame, *, lags=(1, 5, 20), roll_window=20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    XGBoostìš© ì‹œê³„ì—´ í”¼ì²˜ ìƒì„±\n",
    "    \"\"\"\n",
    "    df = df.sort_values(['Sector', 'ds']).copy()\n",
    "\n",
    "    # lag features\n",
    "    for lag in lags:\n",
    "        df[f'y_lag_{lag}'] = df.groupby('Sector')['y'].shift(lag)\n",
    "\n",
    "    # rolling stats\n",
    "    y_shift_1 = df.groupby('Sector')['y'].shift(1)\n",
    "    df[f'y_roll_mean_{roll_window}'] = (\n",
    "        y_shift_1.groupby(df['Sector']).rolling(roll_window).mean().reset_index(level=0, drop=True)\n",
    "    )\n",
    "    df[f'y_roll_std_{roll_window}'] = (\n",
    "        y_shift_1.groupby(df['Sector']).rolling(roll_window).std().reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\" XGBoost í”¼ì²˜ ìƒì„± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " v3 XGBoost í•™ìŠµ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n",
      "    Rank Signal Target ì§€ì›\n"
     ]
    }
   ],
   "source": [
    "def train_v3_xgb_model(\n",
    "    train_data: pd.DataFrame,\n",
    "    prophet_forecast: pd.DataFrame,\n",
    "    feature_cols: list = None,\n",
    "    use_rank_signal: bool = True,\n",
    "    horizon: int = 1,\n",
    "    scale: bool = True,\n",
    "    verbose: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "     v3 XGBoost ëª¨ë¸ í•™ìŠµ (Rank Signal Target)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    use_rank_signal : bool\n",
    "        True: ìƒëŒ€ ê°•ë„ ì”ì°¨ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš© (v3)\n",
    "        False: ì ˆëŒ€ ì”ì°¨ ì‚¬ìš© (v2)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. ë³‘í•©\n",
    "    pf = prophet_forecast[['ds', 'Sector', 'yhat']].copy()\n",
    "    merged = train_data.merge(pf, on=['ds', 'Sector'], how='inner')\n",
    "    \n",
    "    if use_rank_signal:\n",
    "        #  v3: Rank Signal Target\n",
    "        \n",
    "        # 1. ì‹œì¥ í‰ê·  (ì‹¤ì œê°’)\n",
    "        market_y = merged.groupby('ds')['y'].mean().reset_index()\n",
    "        market_y.columns = ['ds', 'market_y']\n",
    "        merged = merged.merge(market_y, on='ds', how='left')\n",
    "        \n",
    "        # 2. ìƒëŒ€ ê°•ë„ (ì‹¤ì œ)\n",
    "        merged['relative_y'] = merged['y'] - merged['market_y']\n",
    "        \n",
    "        # 3. ì‹œì¥ í‰ê·  (ì˜ˆì¸¡)\n",
    "        market_yhat = merged.groupby('ds')['yhat'].mean().reset_index()\n",
    "        market_yhat.columns = ['ds', 'market_yhat']\n",
    "        merged = merged.merge(market_yhat, on='ds', how='left')\n",
    "        \n",
    "        # 4. ìƒëŒ€ ì˜ˆì¸¡\n",
    "        merged['relative_yhat'] = merged['yhat'] - merged['market_yhat']\n",
    "        \n",
    "        # 5. íƒ€ê²Ÿ = ìƒëŒ€ ì”ì°¨\n",
    "        merged['residual'] = merged['relative_y'] - merged['relative_yhat']\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"   v3 Rank Signal Target ì‚¬ìš©\")\n",
    "    else:\n",
    "        # v2: ì ˆëŒ€ ì”ì°¨\n",
    "        merged['residual'] = merged['y'] - merged['yhat']\n",
    "        if verbose:\n",
    "            print(\"   v2 Absolute Residual Target ì‚¬ìš©\")\n",
    "    \n",
    "    # 2. ì‹œê³„ì—´ í”¼ì²˜ ì¶”ê°€\n",
    "    merged = make_xgb_features(merged, lags=(1, 5, 20), roll_window=20)\n",
    "    \n",
    "    # 3. targetì„ ë¯¸ë˜ë¡œ ì´ë™\n",
    "    if horizon > 0:\n",
    "        merged['target'] = merged.groupby('Sector')['residual'].shift(-horizon)\n",
    "    else:\n",
    "        merged['target'] = merged['residual']\n",
    "    \n",
    "    # 4. í”¼ì²˜ ì„¸íŠ¸\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [\n",
    "            'yhat',\n",
    "            'y_lag_1', 'y_lag_5', 'y_lag_20',\n",
    "            'y_roll_std_20',\n",
    "            # v2 í”¼ì²˜\n",
    "            'Avg_Volatility_20d', 'Avg_RSI_14',\n",
    "            'DGS10_level', 'WTI_log', 'Market_Vol',\n",
    "            'Mom_1M', 'Mom_3M', 'Mom_6M',\n",
    "            #  v3 í”¼ì²˜\n",
    "            'RelMom_1M', 'RelMom_3M', 'RelMom_6M', 'RelMom_Accel',\n",
    "            'BigTech_Concentration', 'Hot_Stock_Ratio', 'Relative_Theme_Strength'\n",
    "        ]\n",
    "    \n",
    "    # ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ë§Œ ì„ íƒ\n",
    "    feature_cols_used = [c for c in feature_cols if c in merged.columns]\n",
    "    \n",
    "    # 5. ê²°ì¸¡ì¹˜ ì œê±°\n",
    "    model_df = merged.dropna(subset=feature_cols_used + ['target']).copy()\n",
    "    \n",
    "    X_train = model_df[feature_cols_used].values\n",
    "    y_train = model_df['target'].values\n",
    "    \n",
    "    # 6. ìŠ¤ì¼€ì¼ë§\n",
    "    scaler = None\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # 7. XGBoost í•™ìŠµ\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=2.0,\n",
    "        min_child_weight=5,\n",
    "        random_state=42,\n",
    "        objective='reg:squarederror',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train, verbose=False)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   XGBoost í•™ìŠµ ì™„ë£Œ: {len(model_df):,}ê°œ ìƒ˜í”Œ, {len(feature_cols_used)}ê°œ í”¼ì²˜\")\n",
    "    \n",
    "    return xgb_model, scaler, feature_cols_used\n",
    "\n",
    "print(\" v3 XGBoost í•™ìŠµ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"    Rank Signal Target ì§€ì›\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7. Rolling Loop: ë…„ë„ë³„ ì˜ˆì¸¡ (2021-2025)\n",
    "\n",
    "### ì˜ë„/ëª©ì \n",
    "- 2022-2025ë…„ì„ ìˆœì°¨ì ìœ¼ë¡œ ì˜ˆì¸¡\n",
    "- ê° ë…„ë„ë§ˆë‹¤ ê³¼ê±° ë°ì´í„°ë¡œë§Œ í•™ìŠµ (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)\n",
    "- v3 Prophet + v3 XGBoost (Rank Signal) í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡\n",
    "\n",
    "### í•µì‹¬ ì°¨ì´ì  (v2 â†’ v3)\n",
    "- Prophet: v3 íšŒê·€ë³€ìˆ˜ í¬í•¨\n",
    "- XGBoost: `use_rank_signal=True` (ìƒëŒ€ ê°•ë„ ìµœì í™”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " v3 ì„¹í„°ë³„ ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def predict_sector_year_v3(",
    "    sector_data: pd.DataFrame,",
    "    year: int,",
    "    holidays_df: pd.DataFrame,",
    "    use_rank_signal: bool = True,",
    "    horizon: int = 1",
    "):",
    "    \"\"\"",
    "    v3 ì„¹í„°ë³„ ì˜ˆì¸¡ (08 ë…¸íŠ¸ë¶ ê¸°ë°˜, ì•ˆì •ì )",
    "    \"\"\"",
    "    # 0. ì•ˆì „ì¥ì¹˜",
    "    sectors = sector_data['Sector'].dropna().unique()",
    "    if len(sectors) != 1:",
    "        raise ValueError(f\"sector_data must contain exactly 1 sector, got: {sectors}\")",
    "    sector_name = sectors[0]",
    "    ",
    "    df = sector_data.copy()",
    "    df['ds'] = pd.to_datetime(df['ds'])",
    "    df = df.sort_values('ds').reset_index(drop=True)",
    "    ",
    "    split_date = pd.Timestamp(f\"{year}-01-01\")",
    "    next_year_date = pd.Timestamp(f\"{year+1}-01-01\")",
    "    ",
    "    train = df[df['ds'] < split_date].copy()",
    "    test = df[(df['ds'] >= split_date) & (df['ds'] < next_year_date)].copy()",
    "    ",
    "    if len(train) == 0 or len(test) == 0:",
    "        return None",
    "    ",
    "    # 1. Prophet (ì¶”ì„¸ë§Œ í•™ìŠµ)",
    "    prophet_model = create_v3_prophet_model(holidays_df, train)",
    "    prophet_model.fit(train[['ds', 'y']])",
    "    ",
    "    # ì „ì²´ timelineì— ëŒ€í•´ trend ì˜ˆì¸¡",
    "    fcst = prophet_model.predict(df[['ds']])",
    "    fcst = fcst[['ds', 'yhat']].copy()",
    "    fcst['Sector'] = sector_name",
    "    ",
    "    # 2. Prophet ê²°ê³¼ ë³‘í•© (FULL timeline)",
    "    full = df.merge(fcst, on=['ds', 'Sector'], how='left')",
    "    full['residual'] = full['y'] - full['yhat']",
    "    ",
    "    # 3. XGB í”¼ì²˜ ìƒì„± (FULL timelineì—ì„œ 1íšŒ)",
    "    full_feat = make_xgb_features(full, lags=(1, 5, 20), roll_window=20)",
    "    ",
    "    # 4. target ìƒì„±",
    "    if horizon > 0:",
    "        full_feat['target'] = full_feat.groupby('Sector')['residual'].shift(-horizon)",
    "    else:",
    "        full_feat['target'] = full_feat['residual']",
    "    ",
    "    # 5. train/test ë¶„í• ",
    "    train_feat = full_feat[full_feat['ds'] < split_date].copy()",
    "    test_feat = full_feat[(full_feat['ds'] >= split_date) & (full_feat['ds'] < next_year_date)].copy()",
    "    ",
    "    # 6. XGBoost í”¼ì²˜ ëª©ë¡ (v2 + v3)",
    "    xgb_feature_cols = [",
    "        'yhat',",
    "        'y_lag_1', 'y_lag_5', 'y_lag_20',",
    "        'y_roll_std_20',",
    "        # v2 í”¼ì²˜",
    "        'Avg_Volatility_20d', 'Avg_RSI_14',",
    "        'DGS10_level', 'WTI_log', 'Market_Vol',",
    "        'Mom_1M', 'Mom_3M', 'Mom_6M',",
    "        # v3 í”¼ì²˜",
    "        'RelMom_1M', 'RelMom_3M', 'RelMom_6M', 'RelMom_Accel',",
    "        'BigTech_Concentration', 'Hot_Stock_Ratio', 'Relative_Theme_Strength'",
    "    ]",
    "    ",
    "    # ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ë§Œ ì„ íƒ",
    "    xgb_feature_cols = [c for c in xgb_feature_cols if c in train_feat.columns]",
    "    ",
    "    # 7. XGBoost í•™ìŠµ ë°ì´í„° ì¤€ë¹„",
    "    train_feat_clean = train_feat.dropna(subset=xgb_feature_cols + ['target'])",
    "    test_feat_clean = test_feat.dropna(subset=xgb_feature_cols)",
    "    ",
    "    if len(train_feat_clean) == 0:",
    "        return None",
    "    ",
    "    X_train = train_feat_clean[xgb_feature_cols].values",
    "    y_train = train_feat_clean['target'].values",
    "    ",
    "    # 8. Rank Signal Target (v3 í•µì‹¬)",
    "    if use_rank_signal:",
    "        # ë‚ ì§œë³„ ì‹œì¥ í‰ê·  ê³„ì‚°",
    "        market_y_train = train_feat_clean.groupby('ds')['y'].transform('mean')",
    "        market_yhat_train = train_feat_clean.groupby('ds')['yhat'].transform('mean')",
    "        ",
    "        # ìƒëŒ€ ê°•ë„ ê³„ì‚°",
    "        relative_y_train = train_feat_clean['y'] - market_y_train",
    "        relative_yhat_train = train_feat_clean['yhat'] - market_yhat_train",
    "        ",
    "        # ìƒëŒ€ ì”ì°¨ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©",
    "        y_train = (relative_y_train - relative_yhat_train).values",
    "        ",
    "        # target shift ì ìš©",
    "        if horizon > 0:",
    "            y_train_shifted = []",
    "            for i in range(len(train_feat_clean)):",
    "                if i + horizon < len(train_feat_clean):",
    "                    y_train_shifted.append(y_train[i + horizon])",
    "                else:",
    "                    y_train_shifted.append(np.nan)",
    "            y_train = np.array(y_train_shifted)",
    "            ",
    "            # NaN ì œê±°",
    "            valid_idx = ~np.isnan(y_train)",
    "            X_train = X_train[valid_idx]",
    "            y_train = y_train[valid_idx]",
    "    ",
    "    # 9. ìŠ¤ì¼€ì¼ë§",
    "    scaler = StandardScaler()",
    "    X_train = scaler.fit_transform(X_train)",
    "    ",
    "    # 10. XGBoost í•™ìŠµ",
    "    xgb_model = xgb.XGBRegressor(",
    "        n_estimators=600,",
    "        learning_rate=0.03,",
    "        max_depth=3,",
    "        subsample=0.8,",
    "        colsample_bytree=0.8,",
    "        reg_lambda=2.0,",
    "        min_child_weight=5,",
    "        random_state=42,",
    "        objective='reg:squarederror',",
    "        n_jobs=-1",
    "    )",
    "    xgb_model.fit(X_train, y_train, verbose=False)",
    "    ",
    "    # 11. Test ì˜ˆì¸¡",
    "    if len(test_feat_clean) == 0:",
    "        return None",
    "    ",
    "    X_test = test_feat_clean[xgb_feature_cols].values",
    "    X_test = scaler.transform(X_test)",
    "    resid_hat = xgb_model.predict(X_test)",
    "    ",
    "    test_feat_clean = test_feat_clean.copy()",
    "    test_feat_clean['yhat_xgb_resid_h'] = resid_hat",
    "    ",
    "    # 12. ìµœì¢… ê²°ê³¼",
    "    result = test_feat_clean[['ds', 'Sector', 'y', 'yhat']].copy()",
    "    result['yhat_xgb_resid_h'] = test_feat_clean['yhat_xgb_resid_h'].values",
    "    result = result.rename(columns={'y': 'y_actual', 'yhat': 'yhat_prophet'})",
    "    result['yhat_hybrid_h'] = result['yhat_prophet'] + result['yhat_xgb_resid_h']",
    "    result['yhat_final'] = result['yhat_hybrid_h']",
    "    ",
    "    return result",
    "",
    "print(\"âœ… v3 ì„¹í„°ë³„ ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ (ì•ˆì •ì  ë²„ì „)\")",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:45 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " v3 Rolling Loop ì‹¤í–‰: 2022 ~ 2025\n",
      "======================================================================\n",
      "    ìƒëŒ€ ëª¨ë©˜í…€ í”¼ì²˜ ì‚¬ìš©\n",
      "    Rank Signal Target ì‚¬ìš©\n",
      "    í…Œë§ˆ ê°ì§€ í”¼ì²˜ ì‚¬ìš©\n",
      "\n",
      "======================================================================\n",
      "Test Year 2022 ì˜ˆì¸¡ ì‹œì‘ (Train <= 2021)\n",
      "======================================================================\n",
      "  [1/11] Basic Materials                ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [2/11] Communication Services         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:45 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [3/11] Consumer Cyclical              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:46 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [4/11] Consumer Defensive             ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [5/11] Energy                         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:46 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [6/11] Financial Services             ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [7/11] Healthcare                     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:46 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [8/11] Industrials                    ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [9/11] Real Estate                    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:47 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [10/11] Technology                     ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [11/11] Utilities                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:47 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "\n",
      "======================================================================\n",
      "Test Year 2023 ì˜ˆì¸¡ ì‹œì‘ (Train <= 2022)\n",
      "======================================================================\n",
      "  [1/11] Basic Materials                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:48 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [2/11] Communication Services         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:48 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [3/11] Consumer Cyclical              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:48 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [4/11] Consumer Defensive             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:48 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [5/11] Energy                         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:49 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [6/11] Financial Services             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:49 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [7/11] Healthcare                     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:49 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [8/11] Industrials                    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:50 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [9/11] Real Estate                    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:50 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [10/11] Technology                     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:50 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [11/11] Utilities                      ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "\n",
      "======================================================================\n",
      "Test Year 2024 ì˜ˆì¸¡ ì‹œì‘ (Train <= 2023)\n",
      "======================================================================\n",
      "  [1/11] Basic Materials                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:50 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [2/11] Communication Services         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:51 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [3/11] Consumer Cyclical              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:51 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [4/11] Consumer Defensive             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:52 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [5/11] Energy                         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:52 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [6/11] Financial Services             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:52 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [7/11] Healthcare                     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:52 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [8/11] Industrials                    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:53 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [9/11] Real Estate                    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:53 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [10/11] Technology                     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:53 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [11/11] Utilities                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:54 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "\n",
      "======================================================================\n",
      "Test Year 2025 ì˜ˆì¸¡ ì‹œì‘ (Train <= 2024)\n",
      "======================================================================\n",
      "  [1/11] Basic Materials                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:54 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [2/11] Communication Services         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:55 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [3/11] Consumer Cyclical              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:55 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [4/11] Consumer Defensive             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:55 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [5/11] Energy                         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:56 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [6/11] Financial Services             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:56 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [7/11] Healthcare                     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:57 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [8/11] Industrials                    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:57 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [9/11] Real Estate                    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:58 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [10/11] Technology                     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:58 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "  [11/11] Utilities                      ì˜¤ë¥˜: 'Column not found: yhat'\n",
      "\n",
      " ì˜ˆì¸¡ ì‹¤íŒ¨\n"
     ]
    }
   ],
   "source": [
    "TEST_YEARS = [2022, 2023, 2024, 2025]\n",
    "sectors = sector_panel['Sector'].unique()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\" v3 Rolling Loop ì‹¤í–‰: {TEST_YEARS[0]} ~ {TEST_YEARS[-1]}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"    ìƒëŒ€ ëª¨ë©˜í…€ í”¼ì²˜ ì‚¬ìš©\")\n",
    "print(f\"    Rank Signal Target ì‚¬ìš©\")\n",
    "print(f\"    í…Œë§ˆ ê°ì§€ í”¼ì²˜ ì‚¬ìš©\")\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "for year in TEST_YEARS:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Test Year {year} ì˜ˆì¸¡ ì‹œì‘ (Train <= {year-1})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    year_predictions = []\n",
    "    \n",
    "    for i, sector in enumerate(sectors, 1):\n",
    "        print(f\"  [{i}/{len(sectors)}] {sector:30s} \", end=\"\")\n",
    "        \n",
    "        sector_data = sector_panel[sector_panel['Sector'] == sector].copy()\n",
    "        \n",
    "        try:\n",
    "            pred = predict_sector_year_v3(\n",
    "                sector_data=sector_data,\n",
    "                year=year,\n",
    "                holidays_df=US_HOLIDAYS,\n",
    "                use_rank_signal=True,\n",
    "                horizon=1\n",
    "            )\n",
    "            \n",
    "            if pred is not None and len(pred) > 0:\n",
    "                pred['test_year'] = year\n",
    "                pred['train_end_year'] = year - 1\n",
    "                year_predictions.append(pred)\n",
    "                print(f\"{len(pred)}ì¼ \")\n",
    "            else:\n",
    "                print(\"ë°ì´í„° ë¶€ì¡±\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ì˜¤ë¥˜: {str(e)[:50]}\")\n",
    "    \n",
    "    if year_predictions:\n",
    "        year_df = pd.concat(year_predictions, ignore_index=True)\n",
    "        all_predictions.append(year_df)\n",
    "        print(f\"\\n {year}ë…„ ì˜ˆì¸¡ ì™„ë£Œ: {len(year_df):,} ë ˆì½”ë“œ\")\n",
    "\n",
    "if all_predictions:\n",
    "    df_predictions = pd.concat(all_predictions, ignore_index=True)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\" ì „ì²´ v3 Rolling Loop ì™„ë£Œ\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"ì´ ì˜ˆì¸¡ ë ˆì½”ë“œ: {len(df_predictions):,}\")\n",
    "    print(f\"ì„¹í„° ìˆ˜: {df_predictions['Sector'].nunique()}\")\n",
    "    print(f\"ë…„ë„: {sorted(df_predictions['test_year'].unique().tolist())}\")\n",
    "else:\n",
    "    print(\"\\n ì˜ˆì¸¡ ì‹¤íŒ¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8. í‰ê°€ (Ranking ì¤‘ì‹¬)\n",
    "\n",
    "### ì˜ë„/ëª©ì \n",
    "- **Spearman ìƒê´€ê³„ìˆ˜**: ìˆœìœ„ ì˜ˆì¸¡ë ¥ ì¸¡ì •\n",
    "- **Top-K Hit Rate**: ìƒìœ„ 3/5 ì„¹í„° ì ì¤‘ë¥ \n",
    "- **v3 ëª©í‘œ**: 2023ë…„ Spearman > +0.5 (v2: -0.5)\n",
    "\n",
    "### í‰ê°€ ê¸°ì¤€\n",
    "- MAPE/RMSE: ì°¸ê³ ìš©\n",
    "- **í•µì‹¬**: Spearman, Top-3/5 Hit Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def calculate_ranking_metrics(df, year, pred_col='yhat_final'):\n",
    "    year_data = df[df['test_year'] == year].copy()\n",
    "    if len(year_data) == 0:\n",
    "        return None\n",
    "    \n",
    "    year_data = year_data.sort_values(['Sector', 'ds'])\n",
    "    \n",
    "    def log_return(x):\n",
    "        x = x.dropna()\n",
    "        if len(x) < 2:\n",
    "            return np.nan\n",
    "        return np.exp(x.iloc[-1] - x.iloc[0]) - 1.0\n",
    "    \n",
    "    sector_summary = (\n",
    "        year_data.groupby('Sector')\n",
    "        .agg(\n",
    "            Actual_Return=('y_actual', log_return),\n",
    "            Predicted_Return=(pred_col, log_return),\n",
    "        )\n",
    "        .reset_index()\n",
    "        .dropna(subset=['Actual_Return', 'Predicted_Return'])\n",
    "    )\n",
    "    \n",
    "    sector_summary['Actual_Rank'] = sector_summary['Actual_Return'].rank(ascending=False, method='min')\n",
    "    sector_summary['Predicted_Rank'] = sector_summary['Predicted_Return'].rank(ascending=False, method='min')\n",
    "    sector_summary['Rank_Diff'] = (sector_summary['Actual_Rank'] - sector_summary['Predicted_Rank']).abs()\n",
    "    \n",
    "    spearman_corr, spearman_pvalue = spearmanr(\n",
    "        sector_summary['Predicted_Return'],\n",
    "        sector_summary['Actual_Return']\n",
    "    )\n",
    "    \n",
    "    def top_k_hit_rate(summary_df, k):\n",
    "        k = min(k, len(summary_df))\n",
    "        pred_top_k = set(summary_df.nsmallest(k, 'Predicted_Rank')['Sector'])\n",
    "        actual_top_k = set(summary_df.nsmallest(k, 'Actual_Rank')['Sector'])\n",
    "        return len(pred_top_k & actual_top_k) / k * 100 if k > 0 else np.nan\n",
    "    \n",
    "    top3_hit = top_k_hit_rate(sector_summary, 3)\n",
    "    top5_hit = top_k_hit_rate(sector_summary, 5)\n",
    "    \n",
    "    close_success_rate = (sector_summary['Rank_Diff'] <= 2).mean() * 100 if len(sector_summary) else np.nan\n",
    "    \n",
    "    return {\n",
    "        'Spearman': spearman_corr,\n",
    "        'Spearman_PValue': spearman_pvalue,\n",
    "        'Top3_Hit_Rate': top3_hit,\n",
    "        'Top5_Hit_Rate': top5_hit,\n",
    "        'Close_Success_Rate': close_success_rate,\n",
    "        'Sector_Summary': sector_summary\n",
    "    }\n",
    "\n",
    "print(\" í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ì—ëŸ¬: df_predictionsê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n",
      "==========================================================================================\n",
      "í•´ê²° ë°©ë²•:\n",
      "  1. ìœ„ë¡œ ìŠ¤í¬ë¡¤í•´ì„œ Section 7 (Rolling Loop) ì°¾ê¸°\n",
      "  2. 'TEST_YEARS = [2022, 2023, 2024, 2025]'ë¡œ ì‹œì‘í•˜ëŠ” ì…€ ì‹¤í–‰\n",
      "  3. Rolling Loop ì™„ë£Œ ëŒ€ê¸° (ì•½ 10-20ë¶„)\n",
      "  4. ' ì „ì²´ v3 Rolling Loop ì™„ë£Œ' ë©”ì‹œì§€ í™•ì¸\n",
      "  5. ì´ ì…€(Section 8)ì„ ë‹¤ì‹œ ì‹¤í–‰\n",
      "==========================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "Section 7ì˜ Rolling Loopì„ ë¨¼ì € ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  5. ì´ ì…€(Section 8)ì„ ë‹¤ì‹œ ì‹¤í–‰\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m90\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSection 7ì˜ Rolling Loopì„ ë¨¼ì € ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df_predictions) == \u001b[32m0\u001b[39m:\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m ì—ëŸ¬: df_predictionsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: Section 7ì˜ Rolling Loopì„ ë¨¼ì € ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "# df_predictions ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "if 'df_predictions' not in dir():\n",
    "    print(\" ì—ëŸ¬: df_predictionsê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"=\" * 90)\n",
    "    print(\"í•´ê²° ë°©ë²•:\")\n",
    "    print(\"  1. ìœ„ë¡œ ìŠ¤í¬ë¡¤í•´ì„œ Section 7 (Rolling Loop) ì°¾ê¸°\")\n",
    "    print(\"  2. 'TEST_YEARS = [2022, 2023, 2024, 2025]'ë¡œ ì‹œì‘í•˜ëŠ” ì…€ ì‹¤í–‰\")\n",
    "    print(\"  3. Rolling Loop ì™„ë£Œ ëŒ€ê¸° (ì•½ 10-20ë¶„)\")\n",
    "    print(\"  4. ' ì „ì²´ v3 Rolling Loop ì™„ë£Œ' ë©”ì‹œì§€ í™•ì¸\")\n",
    "    print(\"  5. ì´ ì…€(Section 8)ì„ ë‹¤ì‹œ ì‹¤í–‰\")\n",
    "    print(\"=\" * 90)\n",
    "    raise NameError(\"Section 7ì˜ Rolling Loopì„ ë¨¼ì € ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "if len(df_predictions) == 0:\n",
    "    print(\" ì—ëŸ¬: df_predictionsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
    "    print(\"  Rolling Loopì—ì„œ ì˜ˆì¸¡ì´ ì‹¤íŒ¨í–ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    print(\"  Section 7ì˜ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    raise ValueError(\"df_predictionsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\" v3 ëª¨ë¸ í‰ê°€ ê²°ê³¼ (Ranking ì¤‘ì‹¬)\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "eval_results = []\n",
    "\n",
    "for year in sorted(df_predictions['test_year'].unique()):\n",
    "    print(f\"\\n{'='*90}\")\n",
    "    print(f\"Year {year}\")\n",
    "    print(f\"{'='*90}\")\n",
    "    \n",
    "    year_data = df_predictions[df_predictions['test_year'] == year]\n",
    "    \n",
    "    if len(year_data) == 0:\n",
    "        print(\"  ë°ì´í„° ì—†ìŒ\")\n",
    "        continue\n",
    "    \n",
    "    ranking_metrics = calculate_ranking_metrics(df_predictions, year)\n",
    "    \n",
    "    if ranking_metrics is None:\n",
    "        print(\"  ë­í‚¹ í‰ê°€ ë¶ˆê°€(ë°ì´í„° ë¶€ì¡±)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n[ë­í‚¹ ì§€í‘œ]\")\n",
    "    print(f\"  Spearman Correlation: {ranking_metrics['Spearman']:.4f} (p={ranking_metrics['Spearman_PValue']:.4f})\")\n",
    "    print(f\"  Top-3 Hit Rate: {ranking_metrics['Top3_Hit_Rate']:.1f}%\")\n",
    "    print(f\"  Top-5 Hit Rate: {ranking_metrics['Top5_Hit_Rate']:.1f}%\")\n",
    "    print(f\"  ê·¼ì ‘ ì„±ê³µë¥  (Â±2): {ranking_metrics['Close_Success_Rate']:.1f}%\")\n",
    "    \n",
    "    sector_summary = ranking_metrics['Sector_Summary']\n",
    "    print(f\"\\n[ì„¹í„°ë³„ ìˆœìœ„]\")\n",
    "    print(sector_summary.sort_values('Actual_Rank')[['Sector', 'Actual_Return', 'Actual_Rank', 'Predicted_Rank', 'Rank_Diff']].to_string(index=False))\n",
    "    \n",
    "    eval_results.append({\n",
    "        'Year': year,\n",
    "        'Spearman': ranking_metrics['Spearman'],\n",
    "        'Spearman_PValue': ranking_metrics['Spearman_PValue'],\n",
    "        'Top3_Hit': ranking_metrics['Top3_Hit_Rate'],\n",
    "        'Top5_Hit': ranking_metrics['Top5_Hit_Rate'],\n",
    "        'Close_Success': ranking_metrics['Close_Success_Rate']\n",
    "    })\n",
    "\n",
    "df_eval_summary = pd.DataFrame(eval_results)\n",
    "\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\" ì „ì²´ ìš”ì•½\")\n",
    "print(f\"{'='*90}\")\n",
    "print(df_eval_summary.to_string(index=False))\n",
    "print(f\"\\ní‰ê·  Spearman: {df_eval_summary['Spearman'].mean():.4f}\")\n",
    "print(f\"í‰ê·  Top-3 Hit: {df_eval_summary['Top3_Hit'].mean():.1f}%\")\n",
    "print(f\"í‰ê·  Top-5 Hit: {df_eval_summary['Top5_Hit'].mean():.1f}%\")\n",
    "\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\" v3 ì„±ê³¼ (2023ë…„ ê¸°ì¤€)\")\n",
    "print(f\"{'='*90}\")\n",
    "if 2023 in df_eval_summary['Year'].values:\n",
    "    spearman_2023 = df_eval_summary[df_eval_summary['Year'] == 2023]['Spearman'].values[0]\n",
    "    top3_2023 = df_eval_summary[df_eval_summary['Year'] == 2023]['Top3_Hit'].values[0]\n",
    "    \n",
    "    print(f\"2023 Spearman: {spearman_2023:.4f}\")\n",
    "    print(f\"2023 Top-3 Hit: {top3_2023:.1f}%\")\n",
    "    \n",
    "    if spearman_2023 > 0.5:\n",
    "        print(f\"\\n ëª©í‘œ ë‹¬ì„±! (ëª©í‘œ: >0.5, ì‹¤ì œ: {spearman_2023:.4f})\")\n",
    "    else:\n",
    "        print(f\"\\n ëª©í‘œ ë¯¸ë‹¬ (ëª©í‘œ: >0.5, ì‹¤ì œ: {spearman_2023:.4f})\")\n",
    "else:\n",
    "    print(\"2023ë…„ ë°ì´í„° ì—†ìŒ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(df_eval_summary['Year'], df_eval_summary['Spearman'], \n",
    "        marker='o', linewidth=2, markersize=8, color='#2E86AB')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', linewidth=1)\n",
    "ax.axhline(y=0.5, color='green', linestyle='--', linewidth=1, label='v3 ëª©í‘œ (0.5)')\n",
    "ax.set_xlabel('Year', fontsize=12)\n",
    "ax.set_ylabel('Spearman Correlation', fontsize=12)\n",
    "ax.set_title('ë…„ë„ë³„ Spearman Rank Correlation (v3)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "for i, row in df_eval_summary.iterrows():\n",
    "    ax.text(row['Year'], row['Spearman'] + 0.05, f\"{row['Spearman']:.2f}\", \n",
    "            ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax = axes[1]\n",
    "x = np.arange(len(df_eval_summary))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, df_eval_summary['Top3_Hit'], width, label='Top-3', color='#A23B72')\n",
    "ax.bar(x + width/2, df_eval_summary['Top5_Hit'], width, label='Top-5', color='#F18F01')\n",
    "ax.set_xlabel('Year', fontsize=12)\n",
    "ax.set_ylabel('Hit Rate (%)', fontsize=12)\n",
    "ax.set_title('ë…„ë„ë³„ Top-K Hit Rate (v3)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_eval_summary['Year'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('v3_evaluation_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n ì‹œê°í™” ì™„ë£Œ: v3_evaluation_results.png ì €ì¥\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}